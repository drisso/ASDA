[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics and Data Analysis",
    "section": "",
    "text": "Introduction\nThis is the companion website of the “Advanced Statistics and Data Analysis” course, taught within the “Quantitative and Computational Biosciences” Master’s program at the University of Padova.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#organization-of-this-website",
    "href": "index.html#organization-of-this-website",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Organization of this website",
    "text": "Organization of this website\nThis website is organized as a book. Each class topic is treated in a different chapter.\nIn each Chapter, you will find the slides presented in class and the code used for the practicals.\nYou can use the navigation bar to navigate by topic or the Timeline section below to see the topics in the same order they were presented in class.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#timeline",
    "href": "index.html#timeline",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nDate\nTopic\nType\n\n\n\n\n24/2/2025\nIntro to course and statistics recap\nLecture\n\n\n24/2/2025\nThe Normal Linear Model (part 1)\nLecture\n\n\n…",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#suggested-reading-materials",
    "href": "index.html#suggested-reading-materials",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Suggested reading materials",
    "text": "Suggested reading materials\n\nR resources\n\nR for Data Science book\nTidyverse Skills for Data Science in R\nThe ggplot2 book\nHappy Git with R\nAdvanced R\n\n\n\nStatistics\n\nModern Statistics for Modern Biology",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#work-in-progress",
    "href": "index.html#work-in-progress",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Work in progress",
    "text": "Work in progress\nThe book is a work in progress as we move through the first edition of this class. Please, open issues and contribute pull requests at https://github.com/drisso/ASDA if you find typos or mistakes or if something is missing.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI wish to warmly thank all the authors that have provided open resources related to the topics of this course. In particular, Claus O. Wilke, Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks, Roger D. Peng, Wolfgang Huber, Susan Holmes, Hadley Wickham.\nIn the same spirit, I am sharing openly online this course.\nSpecific resources that can be used as additional readings are noted in the relevant chapter.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Course Introduction",
    "section": "",
    "text": "1.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#exercises",
    "href": "intro.html#exercises",
    "title": "1  Course Introduction",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n\nCreate a scatter-plot of petal length and petal width\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalculate the correlation between these two variables\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalculate the correlation only for the setosa species\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFit a linear model using petal width as a response variable and petal length and species as covariates\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#a-look-ahead",
    "href": "intro.html#a-look-ahead",
    "title": "1  Course Introduction",
    "section": "1.3 A look ahead",
    "text": "1.3 A look ahead\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "2  Data Wrangling and Visualization",
    "section": "",
    "text": "2.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#lab-data-wrangling-and-visualization",
    "href": "dataviz.html#lab-data-wrangling-and-visualization",
    "title": "2  Data Wrangling and Visualization",
    "section": "2.2 Lab: data wrangling and visualization",
    "text": "2.2 Lab: data wrangling and visualization\n\n2.2.1 What’s not covered (i.e., prerequisites)\nBasic R syntax is not covered in this lab, as we assume that you are already familiar with it. Most of the concepts in this first lab should be accessible to peolple with minimal exposure to R (Googling what you don’t remember is allowed – and encouraged!).\nIf you need help getting started with R, this is a good and free tutorial: https://swcarpentry.github.io/r-novice-gapminder/\n\n\n2.2.2 What’s covered (i.e., outline)\nIn this first lab, we will cover how to:\n\nhow to transform, group, and summarize tidy data with the dplyr package\nhow to plot tidy data with the ggplot2 package\n\nNote that this process is sometimes referred to as data wrangling (or data munging). To do this we will make extensive use of the dplyr package.\nThis is not a complete tutorial of the dplyr package. Rather, it’s an introduction of the dplyr syntax.\n\n\n2.2.3 Data wrangling: the dplyr package\nAn important and often time consuming step of any data analysis is “data wrangling,” or the process of cleaning up the dataset. This process is often required before any meaningful data exploration can be carried out.\nAlthough it is possible to plot, analyze and even make inference with messy datasets, you will make your life much easier by cleaning and “tidying” your data as a preliminary step and by saving a cleaned dataset as the starting point of the downstream analyses can save you quite some time in the long run.\nThis often requires removing, adding, transforming variables, as well as filtering, grouping, and ordering observations. Data summary is also often included in the data wrangling definition.\nAlthough base R has all the tools needed to perform these operations, the dplyr add-on package has a nice and concise set of operations that make it easier to perform the typical operations needed at this step.\n\n\n2.2.4 An example dataset: the gapminder package\nTo illustrate the concepts in this lab, we will use the gapminder package, which includes a subset of the Gapminder dataset with data on 142 countries per capita GDP (Gross Domestic Product) and life expectancy between 1952 and 2007.\nThe package is available on CRAN and can be downloaded with the following.\n\ninstall.packages(\"gapminder\")\n\nThis is the way to install packages in R. For this lab we will need the dplyr, tidyr, magrittr, and ggplot2 packages. Although we can install these packages independently, a convenient alternative is to install the tidyverse package.\n\ninstall.packages(\"tidyverse\")\n\nYou will need to install the packages only once, but the packages need to be loaded into R at every new session.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nAfter loading the package, we can have a look at the dataset.\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nNote that this is a tibble, which is a fancy extension of a data.frame.\n\nclass(gapminder)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nOther than having a better method to print to screen, a tibble is a data.frame and in this course we will use the term data.frame whether the object is a tibble or a data.frame.\n\n\n2.2.5 The “pipe” operator\nBefore discussing the main “verbs” available in the dplyr package, we introduce the “pipe” operator. The pipe operator, |&gt;, essentially takes the output of the left-hand side expression and turns it into the first argument of the right-hand side function.\nLet’s consider an example. Assume that we want to know the sum of the first 100 natural numbers. We can first create a vector with the numbers, saving it into a variable, and then compute the sum.\n\nx &lt;- 1:100\nsum(x)\n\n[1] 5050\n\n\nAlternatively, we can generate the vector and directly call the function by nesting the two expressions.\n\nsum(1:100)\n\n[1] 5050\n\n\nThe pipe operator gives yet another alternative.\n\n1:100 |&gt; sum()\n\n[1] 5050\n\n\nAlthough this syntax may seem overly complicated for such small examples, it’s extremely useful when many functions are applied in a sequential way, to avoid the nesting of functions that may create hard to read code.\n\n## nested functions\nset.seed(1547)\nplot(density(rnorm(mean(rnorm(1, mean=10)))))\n\n\n\n\n\n\n\n## pipe\nset.seed(1547)\nrnorm(1, mean=10) |&gt;\n  mean() |&gt;\n  rnorm() |&gt;\n  density() |&gt;\n  plot()\n\n\n\n\n\n\n\nExercise\n\n\n\nUse both the nested syntax and the pipe operator to carry out the following analysis:\n\nUse the rnorm() function to generate 100 data points\nStore the data into a matrix with 2 columns and 50 rows\nCompute the mean of the two columns with the function colMeans()\n\n\n\n\n\n2.2.6 dplyr verbs\nThe five key functions of the dplyr package are the following.\n\nselect(): to select variables, or columns of the data frame.\nfilter(): to select obserations, or rows of the data frame.\narrange(): to order the observations.\nmutate(): to modify a variable or create a new one.\nsummarize(): to summarize the values of a variable.\n\nAnother important function is group_by(), which changes the behavior of the other five functions to operate at the group level rather than on the full dataset.\nWe will illustrate the five verbs by trying to answer a few interesting questions with the gapminder dataset:\n\nWhat were the 5 richest countries in Europe in 1997?\nWhat was the total GDP of Japan in 1962?\nWhat was the average life expectancy for each continent in 2007?\nHow did the average life expectancy of each continent change each year between 1962 and 1997?\n\n\n2.2.6.1 Five richest European countries in 1997\nTo answer this question we need to:\n\nSelect the observations corresponding to European countries in the year 1997;\nselect the variables related to gdp and country;\norder the countries by gdp.\n\n\ngapminder |&gt;\n  filter(continent == \"Europe\") |&gt;\n  filter(year == 1997) |&gt;\n  dplyr::select(country, gdpPercap) |&gt;\n  arrange(desc(gdpPercap)) |&gt;\n  head(n=5)\n\n# A tibble: 5 × 2\n  country     gdpPercap\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Norway         41283.\n2 Switzerland    32135.\n3 Netherlands    30246.\n4 Denmark        29804.\n5 Austria        29096.\n\n\n\n\n2.2.6.2 Total GDP of Japan in 1962\nTo answer this question we need to:\n\nSelect the observations corresponding to Japan in the year 1962;\nmultiply per capita GDP by total population.\n\n\ngapminder |&gt;\n  filter(country == \"Japan\" & year == 1962) |&gt;\n  mutate(totalGDP = gdpPercap * pop) |&gt;\n  pull(totalGDP)\n\n[1] 630251873021\n\n\n\n\n2.2.6.3 Average life expectancy for each continent in 2007\nTo answer this question we need to:\n\nSelect the observations corresponding to the year 2007;\ngroup the data by continent;\ncompute the average.\n\n\ngapminder |&gt;\n  filter(year == 2007) |&gt;\n  group_by(continent) |&gt;\n  summarize(averageExp = mean(lifeExp))\n\n# A tibble: 5 × 2\n  continent averageExp\n  &lt;fct&gt;          &lt;dbl&gt;\n1 Africa          54.8\n2 Americas        73.6\n3 Asia            70.7\n4 Europe          77.6\n5 Oceania         80.7\n\n\n\n\n2.2.6.4 Life expectancy per continent 1962-1997\n\ngapminder |&gt;\n  filter(year &gt;= 1962 & year &lt;= 1997) |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp))\n\n# A tibble: 40 × 3\n# Groups:   continent [5]\n   continent  year averageExp\n   &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n 1 Africa     1962       43.3\n 2 Africa     1967       45.3\n 3 Africa     1972       47.5\n 4 Africa     1977       49.6\n 5 Africa     1982       51.6\n 6 Africa     1987       53.3\n 7 Africa     1992       53.6\n 8 Africa     1997       53.6\n 9 Americas   1962       58.4\n10 Americas   1967       60.4\n# ℹ 30 more rows\n\n\nThis is too much information to see on the screen. We will see in the next session how to visualize the data in a plot.\nSome other useful functions defined in the dplyr package are the following.\n\nslice: select rows by position.\ncase_when and ifelse for conditional operators.\ntransmute: a mutate that drops existing variables.\ndo: perform arbitrary operations.\n\n\n\n\n\n\n\nExercise\n\n\n\nUse the dplyr verbs to answer the following questions.\n\nWhat were the top 5 most populous countries in Asia in 1952? And in 2007?\nWhat is the average GDP of European countries in 1982?\nWhat was the minimum and maximum population of Italy across the study years?\n[advanced] What is the average increase in per capita GDP per continent between 1952 and 2007?\n\n\n\n\n\n\n2.2.7 Plotting tidy data: the ggplot2 package\nAlthough base R has a plotting system, there are addon packages that define other plotting functions. One of them is the ggplot2 package. ggplot2 defines a “grammar of graphics” as a consistent way to create very different plots.\nIn my real analysis I use a mix of base graphics and ggplot2 as I find that both systems have pros and cons and each is more appropriate for certain types of data and/or plots. Here, we assume that you are already familiar with base graphics, or that you can learn it on your own, and we focus on ggplot2.\nLet’s start from the last example of the previous section. We want to explore the trends in life expectancy for each continent over the years. We can use group_by and summarize to compute the average life expectancy per each continent between 1962 and 2007. But how can we visualize it? Here is where the ggplot2 package comes into play.\n\ngapminder |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp)) |&gt;\n  ggplot(aes(x = year, y = averageExp, group=continent,\n             color=continent)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe ggplot function has two main arguments:\n\na dataset that contains the data (in the example above passed with the pipe operator);\nan “aesthetic mapping” created by the aes() function.\n\nNote that in addition to specifying the x and y axis, we also specified a grouping variable and a variable that defines the color of the lines.\nHere, we plotted the data for the continent in the same plot with different colors. This is fine when plotting a few lines, but what if we wanted to plot the data for each country rather than each continent? A simple modification to our code let us do just that.\n\ntheme_set(theme_minimal())\n\ngapminder |&gt;\n  ggplot(aes(x = year, y = lifeExp, group=country,\n             color=continent)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis is not great because of overplotting. In this situations, a popular device is the use of facets.\n\ngapminder |&gt;\n  ggplot(aes(x = year, y = lifeExp, group=country,\n             color=continent)) +\n  geom_line() +\n  facet_wrap(~continent, nrow = 2)\n\n\n\n\n\n\n\n\nThis plot can be further improved by plotting only the average curve and the range for each continent.\n\ngapminder |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp), \n            minExp = min(lifeExp),\n            maxExp = max(lifeExp)) |&gt;\n  ggplot(aes(x = year, y = averageExp, group=continent,\n             fill=continent)) +\n  geom_ribbon(aes(ymin = minExp, ymax = maxExp, alpha = 0.5)) +\n  geom_line() + theme(legend.position=\"none\") +\n  facet_wrap(~continent, nrow = 2)\n\n\n\n\n\n\n\n\nLet’s say that we are interested in the relation between life expectancy and GDP. Let’s have a look at the first available year, 1962. The best way to visualize the data to answer this question is with a scatterplot, available in ggplot2 via the geom_point.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis is not a very satisfying plot. The data looks better if we plot the x axis on the log scale.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() + scale_x_log10()\n\n\n\n\n\n\n\n\nTo extract more information from this analysis, we can also color the points by continent and have the size of the point proportional to the country’s population.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp, color=continent, size=pop)) +\n  geom_point() + scale_x_log10()\n\n\n\n\n\n\n\n\nAnalogously, we can change the shape and transparency of the points with the parameters shape and alpha.\nHere we have seen three geometries, the line, the point, and the “ribbon”. Other useful geometries are geom_boxplot, geom_histogram, geom_density, etc. See the cheat sheet for all the geometries.\nThe last version of the plot is very close to an optimal representation of the data. This graphical summary raises some interesting questions, e.g., what is the country that sits by itself on the right-hand side of the plot? This is by far the richest country per capita, but its life expectancy is not very high.\nOne solution is to add labels.\n\nlibrary(ggrepel)\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp, color=continent, label=country)) +\n    geom_point(aes(size = pop)) +\n    geom_label_repel() + scale_x_log10()\n\n\n\n\n\n\n\n\nThis is a great plot for 1962, but how do things change across the years? We could use facets to display a few years, but if we are going to explore all years, we need a dynamic plot.\n\nlibrary(gganimate)\ngap &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +\n  geom_point(alpha = 0.7, show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  scale_x_log10() +\n  facet_wrap(~continent, ncol = 5) +\n  # animate it!\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  theme_bw()\n\ngap\n\n\n\n\n\n\n\n\nFinally, if we want to visualize the distribution of life expectancy, we can do that by using histograms, boxplots, violin plots or ridgeline plots.\n\nggplot(gapminder, aes(x=lifeExp)) +\n    geom_histogram(fill = \"dodgerblue\")\n\n\n\n\n\n\n\nggplot(gapminder, aes(x=continent, y=lifeExp, fill=continent)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(gapminder, aes(x=continent, y=lifeExp, fill=continent)) +\n    geom_violin()\n\n\n\n\n\n\n\nlibrary(ggridges)\nggplot(gapminder, aes(y=continent, x=lifeExp, fill=continent)) +\n    geom_density_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse ggplot to produce the following graphs.\n\nVisualize the distribution of life expectancy as a histogram + a density line (hint: geom_density).\nUse facets and the geometry of your choice to visualize the distribution of life expectancy for each continent and each year.\nReproduce the ridgeline plot above, changing the colors to be those stored in the continent_colors variable.\nCreate a scatterplot of life expectancy vs. GDP per capita across all years and continents, using 2d density (geom_density_2d) or hexagonal binning (geom_hex) to avoid overplotting.\nCreate a scatterplotof life expectancy vs. GDP per capita in 2007, with the point size proportional to population, color-coded by country, faceted by continent, adding text labels (hint: remove legend with theme(legend.position = \"none\")).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#further-reading",
    "href": "dataviz.html#further-reading",
    "title": "2  Data Wrangling and Visualization",
    "section": "2.3 Further reading",
    "text": "2.3 Further reading\n\nClaus O. Wilke. Fundamentals of Data Visualization\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 3\nCarrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng Tidyverse Skills for Data Science",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "",
    "text": "3.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#lab-tidy-data-in-r",
    "href": "dataviz2.html#lab-tidy-data-in-r",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.2 Lab: tidy data in R",
    "text": "3.2 Lab: tidy data in R\n\n3.2.1 What’s not covered (i.e., prerequisites)\nBasic R syntax is not covered in this lab, as we assume that you are already familiar with it. Most of the concepts in this first lab should be accessible to peolple with minimal exposure to R (Googling what you don’t remember is allowed – and encouraged!).\nIf you need help getting started with R, this is a good and free tutorial: https://swcarpentry.github.io/r-novice-gapminder/\n\n\n3.2.2 What’s covered (i.e., outline)\nIn this second lab, we will cover how to:\n\nimport the data into R\nclean the data and transform them into tidy data\nwork with more complex data structures\n\n\n\n3.2.3 Import and export the data\nThere are several functions to import the data into R depending on the format in which the data live outside of R. First, we will assume that the data exist in a standard text format, such as tab-delimited or comma-separated. We will briefly mention some packages that can be used to import data in MS Excel, Matlab, and other non-standard formats.\nBase R has many useful functions for the import of flat data tables, read.table and read.cvs among others. However, we will use the readr package here and in particular we will illustrate the read_csv, read_tsv and read_delim functions. These functions are faster and offer more flexibility compared to the base R counterparts. However, sometimes using read.table can be useful, for instance when one wants to use row.names.\n\n3.2.3.1 Read the data into R\nOften the data come in the form of csv files (comma-separated values). Excel spreadsheet can also be saved to csv files, making it a useful format to read into R data generated with Excel.\nAnother popular format is tsv files (tab-separated values). This is for instance how the gapminder data has been saved to file in the gapminder package.\nFinally, read_delim can be used to read files whose fields are separated by any character (most commonly a space) which need to be specified in the delim argument.\nHere, we will illustrate the use of read_tsv since the other two functions are very similar.\nFirst, we need a string that describes the location on disk of the file that we want to import in R. Note that the string will change depending on your OS.\n\nlibrary(readr)\ngap_tsv &lt;- system.file(\"extdata/gapminder.tsv\", package = \"gapminder\")\ngap_tsv\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/gapminder/extdata/gapminder.tsv\"\n\ngap &lt;- read_tsv(gap_tsv)\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngap\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nNote how the read_tsv function printed a message with the type that it inferred from each of the columns.\n\n\n3.2.3.2 Write the data to file\nSimilarly to read_tsv and read_csv, the functions write_tsv and write_csv can be used to write the R tables to a file on disk.\nNote that writing flat tables to csv or tsv files is the easiest and most reproducible approach. For complex data structures, say an histogram or a clustering tree, one can use the function save to save a binary R representation of the object on a .rda file. Similarly, the function load will read .rda files back into R.\n\n\n3.2.3.3 Additional resources for data import/export\nSometimes you may need to import data from different formats, e.g., because your collaborator has the data in a MS Excel file or because you are continuing in R an analysis started with SPSS, Stata, or SAS.\nFortunately, R has many packages that can be used for data import/export:\n\nhaven can be used to read SPSS, Stata, and SAS files.\nreadxl to read excel files (both .xls and .xlsx).\nDBI to import data from a variety of databases (advanced).\njsonlite to import json files\nxml2 to import XML files.\n\n\n\n\n3.2.4 Tidy the data: the tidyr package\nOften, a large amount of time is spent on “cleaning” the data, in what is sometimes referred to data wrangling.\nThis is because often the data are plagued by missing or implausible values, either because of technical issues with the data collection or because of human error. Moreover, data are often recorded in a way that is useful for storing them, but not ideal for analyzing them.\nThe objective of data wrangling is to take a messy dataset and make it tidy, so that it becomes easier to work with.\n\n3.2.4.1 What are tidy data?\nThe concept of tidy data was introduced by Hadley Wickham in his seminal paper.\nThere are three fundamental rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThe advantage of tidy data is both conceptual and practical: it allows you to have consistency between datasets and to use tools designed for this data structure. Moreover, because R works extremely well with vectorized operation it is very efficient to operate on the columns of a data frame.\n\n\n3.2.4.2 Examples of tidy and untidy data\nTo illustrate the concept of tidy data, we can use the examples in the EDAWR package. First we need to download and install it from Github with the devtools package.\n\nlibrary(remotes)\ninstall_github(\"rstudio/EDAWR\")\n\n\nstormscasespollution\n\n\n\nlibrary(EDAWR)\nstorms\n\n    storm wind pressure       date\n1 Alberto  110     1007 2000-08-03\n2    Alex   45     1009 1998-07-27\n3 Allison   65     1005 1995-06-03\n4     Ana   40     1013 1997-06-30\n5  Arlene   50     1010 1999-06-11\n6  Arthur   45     1010 1996-06-17\n\n\n\n\n\ncases\n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\n\n\n\n\npollution\n\n      city  size amount\n1 New York large     23\n2 New York small     14\n3   London large     22\n4   London small     16\n5  Beijing large    121\n6  Beijing small     56\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake some time to list and describe the variables present in each datasets. Which of these datasets is tidy?\n\n\nNote that somebody’s tidy data is someone else’s untidy data. For instance in the pollution example, one could argue that the two variables in addition to city are: particle size, and particle amount. In this case the dataset is tidy. But another argument is that the variables are: amount of small particles and amount of large particles. In this case the dataset is untidy.\nFor this reason, I actually prefer another terminology: long data and wide data. The pollution dataset is stored in long format, while the cases dataset is stored in wide format.\nTo switch between long and wide format, the tidyr package provide two extremely useful functions: piveot_longer() and pivot_wider().\n\n\n3.2.4.3 Make your data “tall”: pivot_longer()\nThe function pivot_longer() can be used to transform the data from wide to tall.\nThe cases data frame is in wide form. If we want to make it tall, we can use the following command.\n\nlibrary(tidyr)\npivot_longer(cases, names_to = \"year\", values_to = \"n\", cols = 2:4)\n\n# A tibble: 9 × 3\n  country year      n\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n1 FR      2011   7000\n2 FR      2012   6900\n3 FR      2013   7000\n4 DE      2011   5800\n5 DE      2012   6000\n6 DE      2013   6200\n7 US      2011  15000\n8 US      2012  14000\n9 US      2013  13000\n\n\nThe names_to and values_to arguments are simply the names of the new columns that will have the variable values. The cols argument specify the columns that contain the data and that should be transformed.\n\n\n3.2.4.4 Make your data “wide”: pivot_wider()\nAnalogously, the pivot_wider() function let us go back to wide data from tall data.\nIf we pivot_wider a table that we previously pivot_longered, we should return to the original data representation.\n\ncases\n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\npivot_longer(cases, names_to = \"year\", values_to = \"n\", cols = 2:4)  %&gt;%\n  pivot_wider(., names_from = \"year\", values_from = \"n\")\n\n# A tibble: 3 × 4\n  country `2011` `2012` `2013`\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 FR        7000   6900   7000\n2 DE        5800   6000   6200\n3 US       15000  14000  13000\n\n\nAs we mentioned, sometimes the wide format is the tidy format. This could be the case for the pollution data, which we can transform in the following way.\n\npivot_wider(pollution, names_from = \"size\", values_from = \"amount\")\n\n# A tibble: 3 × 3\n  city     large small\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 New York    23    14\n2 London      22    16\n3 Beijing    121    56\n\n\nAdditional functions useful to transform the data are the separate() and unite() functions. You can find out what they do as an exercize.\n\n\n\n\n\n\nExercise\n\n\n\nExplore the dataset airquality:\n\nWhat are the variables? What are the observations?\nTransform it into a taller dataset, so that it will look like this:\n\n\n\n# A tibble: 612 × 4\n   Month   Day variable value\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     5     1 Ozone     41  \n 2     5     1 Solar.R  190  \n 3     5     1 Wind       7.4\n 4     5     1 Temp      67  \n 5     5     2 Ozone     36  \n 6     5     2 Solar.R  118  \n 7     5     2 Wind       8  \n 8     5     2 Temp      72  \n 9     5     3 Ozone     12  \n10     5     3 Solar.R  149  \n# ℹ 602 more rows\n\n\n\nStarting from the created dataset use pivot_wider to turn it back to the original\n\n\n\n\n\n3.2.4.5 Joining tables\nIn the tidy data framework, each table has one row per observational unit and one column per variable. But what if our analysis involves multiple observational unit types? In that case, there might be one table per observational unit types and you might need to join tables to perform certain data analyses.\nThe dplyr package contains several join functions. See ?inner_join for a full list. Here, we will cover only the inner and full joins, with the other operations left for homework exercises.\nTo illustrate joins, we will use the songs and artists datasets available in the EDAWR package. As you can see the observational units are quite different in the two tables, but nonetheless we might need both sets of variables in a single analysis.\n\nsongs\n\n                 song  name\n1 Across the Universe  John\n2       Come Together  John\n3      Hello, Goodbye  Paul\n4           Peggy Sue Buddy\n\nartists\n\n    name  plays\n1 George  sitar\n2   John guitar\n3   Paul   bass\n4  Ringo  drums\n\n\nThe inner join only returns those observations that are present in both datasets. You can think of it as a “intersection.”\n\nlibrary(dplyr)\ninner_join(songs, artists, by = \"name\")\n\n                 song name  plays\n1 Across the Universe John guitar\n2       Come Together John guitar\n3      Hello, Goodbye Paul   bass\n\n\nCoversely, the full join returns all observations that are present in either dataset. You can think of it as a “union.”\n\nfull_join(songs, artists, by = \"name\")\n\n                 song   name  plays\n1 Across the Universe   John guitar\n2       Come Together   John guitar\n3      Hello, Goodbye   Paul   bass\n4           Peggy Sue  Buddy   &lt;NA&gt;\n5                &lt;NA&gt; George  sitar\n6                &lt;NA&gt;  Ringo  drums\n\n\nFinally, the left and right join keep the left or right observations, respectively.\n\nleft_join(songs, artists)\n\n                 song  name  plays\n1 Across the Universe  John guitar\n2       Come Together  John guitar\n3      Hello, Goodbye  Paul   bass\n4           Peggy Sue Buddy   &lt;NA&gt;\n\nright_join(songs, artists)\n\n                 song   name  plays\n1 Across the Universe   John guitar\n2       Come Together   John guitar\n3      Hello, Goodbye   Paul   bass\n4                &lt;NA&gt; George  sitar\n5                &lt;NA&gt;  Ringo  drums\n\n\nNote how we can imply the “by” argument if we want to use all the common variable names between the two tables.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the nycflights13 package to explore the four datasets flights, airlines, weather, and planes.\n\nJoin the two tables in a way that each flight contains the names of the airline\nJoin the resulting table with the weather information\nJoin the resulting table with the airplane information\nUse tidyverse functions to extract the complete information (including weather and airplane info) of UA1545 flight departed on Jan 1st 2013.\nPlot departure delay versus wind speed: is there a relation between these variables? (Hint: avoid overplotting)\n\n\n\n\n\n\n3.2.5 The “tidyverse” vs. “base R”\nAs you look at examples or find answers to your questions online, you will notice that people refer to the set of packages that includes dplyr, tidyr, and ggplot2 as the tidyverse. You will often see people asking for a “tidyverse” way of doing something or for a “base R” way of doing something.\nThere is no right or wrong way to achieve something in R, but considerations about code readability and efficiency may be important depending on the application.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#howemork-1",
    "href": "dataviz2.html#howemork-1",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.3 Howemork 1",
    "text": "3.3 Howemork 1\nHealth policy in the United States is complicated, and several forms of healthcare coverage exist, including both coverage by federal goverment-led healthcare policy, and by private insurance companies. Before making any inference about the relationship between health condition and health policy, it is important for us to have a general idea about healthcare economics in the United States. Thus, we are interested in getting sense of healthcare coverage and healthcare spending across States. More specifically, the questions are:\n\nIs there a relationship between healthcare coverage and healthcare spending in the United States?\nHow does the spending distribution change across geographic regions in the United States?\nDoes the relationship between healthcare coverage and healthcare spending in the United States change from 2013 to 2014?\n\n\n3.3.1 Guided solution\n\nRead-in the data: use the read_csv function (readr package) to read the healthcare-coverage.cvs and the healthcare-spending.csv files (found at https://github.com/opencasestudies/ocs-healthexpenditure).\nLoad the state information found in the state datasets (datasets package). Note that you need to manually add information about the District of Columbia.\nAdd the abbreviation and the region of each state to the coverage dataset.\nJoin the coverage and spending datasets.\nUse ggplot to produce a scatterplot of the proportion of coverage by state vs. the spending per capita. Color code the points by region and add state abbreviations as labels. Use facet to stratify the analysis by type of coverage.\nUse ggplot to create a boxplot of spending per capita stratified by region.\nRepeat the graph in point 5 but faceting by year, in addition to coverage type. Hint: use the facet_grid() function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#further-reading",
    "href": "dataviz2.html#further-reading",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.4 Further reading",
    "text": "3.4 Further reading\n\nClaus O. Wilke. Fundamentals of Data Visualization\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 3\nCarrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng Tidyverse Skills for Data Science",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#footnotes",
    "href": "dataviz2.html#footnotes",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "",
    "text": "This homework is one of the Open Case Studies. Please try to solve this yourself before looking at the solution there.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod.html",
    "href": "linmod.html",
    "title": "4  Statistical modeling",
    "section": "",
    "text": "4.1 Lecture Slides",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "linmod.html#lecture-slides",
    "href": "linmod.html#lecture-slides",
    "title": "4  Statistical modeling",
    "section": "",
    "text": "4.1.1 Statistical modeling\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\n\n4.1.2 Linear models\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "linmod.html#lab-linear-models",
    "href": "linmod.html#lab-linear-models",
    "title": "4  Statistical modeling",
    "section": "4.2 Lab: linear models",
    "text": "4.2 Lab: linear models",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "linmod.html#homework-1",
    "href": "linmod.html#homework-1",
    "title": "4  Statistical modeling",
    "section": "4.3 Homework 1",
    "text": "4.3 Homework 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "linmod.html#further-reading",
    "href": "linmod.html#further-reading",
    "title": "4  Statistical modeling",
    "section": "4.4 Further reading",
    "text": "4.4 Further reading\n\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  },
  {
    "objectID": "linmod.html#footnotes",
    "href": "linmod.html#footnotes",
    "title": "4  Statistical modeling",
    "section": "",
    "text": "This homework is one of the Open Case Studies. Please try to solve this yourself before looking at the solution there.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical modeling</span>"
    ]
  }
]