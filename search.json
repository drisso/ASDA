[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Statistics and Data Analysis",
    "section": "",
    "text": "Introduction\nThis is the companion website of the “Advanced Statistics and Data Analysis” course, taught within the “Quantitative and Computational Biosciences” Master’s program at the University of Padova.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#organization-of-this-website",
    "href": "index.html#organization-of-this-website",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Organization of this website",
    "text": "Organization of this website\nThis website is organized as a book. Each class topic is treated in a different chapter.\nIn each Chapter, you will find the slides presented in class and the code used for the practicals.\nYou can use the navigation bar to navigate by topic or the Timeline section below to see the topics in the same order they were presented in class.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#timeline",
    "href": "index.html#timeline",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Timeline",
    "text": "Timeline\n\nLectures\n\n\n\nWeek\nDate\nTopic\nHours\n\n\n\n\n1\n24/2/2025\nIntro to course and statistics recap\n2\n\n\n\n25/2/2025\nData wrangling and visualization\n2\n\n\n\n26/2/2025\nVisualizing distributions\n2\n\n\n\n27/2/2025\nVisualizing multiple variables\n2\n\n\n\n28/2/2025\nStatistical modeling\n2\n\n\n2\n10/3/2025\nLeast squares regression\n2\n\n\n\n11/3/2025\nLinear models (part 1)\n4\n\n\n\n12/3/2025\nLinear models (part 2)\n2\n\n\n\n13/3/2025\nConfounding and interactions\n2\n\n\n\n14/3/2025\nExperimental design (part 1)\n2\n\n\n3\n17/3/2025\nExperimental design (part 2)\n2\n\n\n\n18/3/2025\nGenralized Linear Models\n2\n\n\n\n\n\nLabs\n\n\n\nWeek\nDate\nTopic\nHours\n\n\n\n\n1\n26/2/2025\nData wrangling and visualization (part 1)\n4\n\n\n\n27/2/2025\nData wrangling and visualization (part 2)\n4\n\n\n\n28/2/2025\nData wrangling and visualization (part 3)\n4\n\n\n2\n13/3/2025\nLinear models (part 1)\n4\n\n\n\n14/3/2025\nLinear models (part 2)\n4\n\n\n3\n17/3/2025\nLinear models (part 3)\n4",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#suggested-reading-materials",
    "href": "index.html#suggested-reading-materials",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Suggested reading materials",
    "text": "Suggested reading materials\n\nR resources\n\nR for Data Science book\nTidyverse Skills for Data Science in R\nThe ggplot2 book\nHappy Git with R\nAdvanced R\n\n\n\nStatistics\n\nModern Statistics for Modern Biology\nData Analysis for the Life Sciences",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#work-in-progress",
    "href": "index.html#work-in-progress",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Work in progress",
    "text": "Work in progress\nThe book is a work in progress as we move through the first edition of this class. Please, open issues and contribute pull requests at https://github.com/drisso/ASDA if you find typos or mistakes or if something is missing.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Advanced Statistics and Data Analysis",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI wish to warmly thank all the authors that have provided open resources related to the topics of this course. In particular, the following people either directly or indirectly inspired the materials developed for this course: Claus O. Wilke, Carrie Wright, Shannon E. Ellis, Stephanie C. Hicks, Roger D. Peng, Wolfgang Huber, Susan Holmes, Hadley Wickham, Lieven Clement, Milan Malfait, Karl Broman, Rafael Irizarry, Mike Love, Jeff Leek, Brian Caffo, Charlotte Soneson.\nIn the same spirit, I am sharing openly online this course.\nSpecific resources that can be used as additional readings are noted in the relevant chapters.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Course Introduction",
    "section": "",
    "text": "1.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#exercises",
    "href": "intro.html#exercises",
    "title": "1  Course Introduction",
    "section": "1.2 Exercises",
    "text": "1.2 Exercises\n\nCreate a scatter-plot of petal length and petal width\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalculate the correlation between these two variables\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalculate the correlation only for the setosa species\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFit a linear model using petal width as a response variable and petal length and species as covariates\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#a-look-ahead",
    "href": "intro.html#a-look-ahead",
    "title": "1  Course Introduction",
    "section": "1.3 A look ahead",
    "text": "1.3 A look ahead\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course Introduction</span>"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "2  Data Wrangling and Visualization",
    "section": "",
    "text": "2.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#lab-data-wrangling-and-visualization",
    "href": "dataviz.html#lab-data-wrangling-and-visualization",
    "title": "2  Data Wrangling and Visualization",
    "section": "2.2 Lab: data wrangling and visualization",
    "text": "2.2 Lab: data wrangling and visualization\n\n2.2.1 What’s not covered (i.e., prerequisites)\nBasic R syntax is not covered in this lab, as we assume that you are already familiar with it. Most of the concepts in this first lab should be accessible to peolple with minimal exposure to R (Googling what you don’t remember is allowed – and encouraged!).\nIf you need help getting started with R, this is a good and free tutorial: https://swcarpentry.github.io/r-novice-gapminder/\n\n\n2.2.2 What’s covered (i.e., outline)\nIn this first lab, we will cover how to:\n\nhow to transform, group, and summarize tidy data with the dplyr package\nhow to plot tidy data with the ggplot2 package\n\nNote that this process is sometimes referred to as data wrangling (or data munging). To do this we will make extensive use of the dplyr package.\nThis is not a complete tutorial of the dplyr package. Rather, it’s an introduction of the dplyr syntax.\n\n\n2.2.3 Data wrangling: the dplyr package\nAn important and often time consuming step of any data analysis is “data wrangling,” or the process of cleaning up the dataset. This process is often required before any meaningful data exploration can be carried out.\nAlthough it is possible to plot, analyze and even make inference with messy datasets, you will make your life much easier by cleaning and “tidying” your data as a preliminary step and by saving a cleaned dataset as the starting point of the downstream analyses can save you quite some time in the long run.\nThis often requires removing, adding, transforming variables, as well as filtering, grouping, and ordering observations. Data summary is also often included in the data wrangling definition.\nAlthough base R has all the tools needed to perform these operations, the dplyr add-on package has a nice and concise set of operations that make it easier to perform the typical operations needed at this step.\n\n\n2.2.4 An example dataset: the gapminder package\nTo illustrate the concepts in this lab, we will use the gapminder package, which includes a subset of the Gapminder dataset with data on 142 countries per capita GDP (Gross Domestic Product) and life expectancy between 1952 and 2007.\nThe package is available on CRAN and can be downloaded with the following.\n\ninstall.packages(\"gapminder\")\n\nThis is the way to install packages in R. For this lab we will need the dplyr, tidyr, magrittr, and ggplot2 packages. Although we can install these packages independently, a convenient alternative is to install the tidyverse package.\n\ninstall.packages(\"tidyverse\")\n\nYou will need to install the packages only once, but the packages need to be loaded into R at every new session.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nAfter loading the package, we can have a look at the dataset.\n\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nNote that this is a tibble, which is a fancy extension of a data.frame.\n\nclass(gapminder)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nOther than having a better method to print to screen, a tibble is a data.frame and in this course we will use the term data.frame whether the object is a tibble or a data.frame.\n\n\n2.2.5 The “pipe” operator\nBefore discussing the main “verbs” available in the dplyr package, we introduce the “pipe” operator. The pipe operator, |&gt;, essentially takes the output of the left-hand side expression and turns it into the first argument of the right-hand side function.\nLet’s consider an example. Assume that we want to know the sum of the first 100 natural numbers. We can first create a vector with the numbers, saving it into a variable, and then compute the sum.\n\nx &lt;- 1:100\nsum(x)\n\n[1] 5050\n\n\nAlternatively, we can generate the vector and directly call the function by nesting the two expressions.\n\nsum(1:100)\n\n[1] 5050\n\n\nThe pipe operator gives yet another alternative.\n\n1:100 |&gt; sum()\n\n[1] 5050\n\n\nAlthough this syntax may seem overly complicated for such small examples, it’s extremely useful when many functions are applied in a sequential way, to avoid the nesting of functions that may create hard to read code.\n\n## nested functions\nset.seed(1547)\nplot(density(rnorm(mean(rnorm(1, mean=10)))))\n\n\n\n\n\n\n\n## pipe\nset.seed(1547)\nrnorm(1, mean=10) |&gt;\n  mean() |&gt;\n  rnorm() |&gt;\n  density() |&gt;\n  plot()\n\n\n\n\n\n\n\nExercise\n\n\n\nUse both the nested syntax and the pipe operator to carry out the following analysis:\n\nUse the rnorm() function to generate 100 data points\nStore the data into a matrix with 2 columns and 50 rows\nCompute the mean of the two columns with the function colMeans()\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\n## nested syntax\ncolMeans(matrix(rnorm(100), ncol=2, nrow=50))\n\n## pipe operator\nrnorm(100) |&gt; matrix(ncol=2, nrow=50) |&gt; colMeans()\n\n\n\n\n\n\n2.2.6 dplyr verbs\nThe five key functions of the dplyr package are the following.\n\nselect(): to select variables, or columns of the data frame.\nfilter(): to select obserations, or rows of the data frame.\narrange(): to order the observations.\nmutate(): to modify a variable or create a new one.\nsummarize(): to summarize the values of a variable.\n\nAnother important function is group_by(), which changes the behavior of the other five functions to operate at the group level rather than on the full dataset.\nWe will illustrate the five verbs by trying to answer a few interesting questions with the gapminder dataset:\n\nWhat were the 5 richest countries in Europe in 1997?\nWhat was the total GDP of Japan in 1962?\nWhat was the average life expectancy for each continent in 2007?\nHow did the average life expectancy of each continent change each year between 1962 and 1997?\n\n\n2.2.6.1 Five richest European countries in 1997\nTo answer this question we need to:\n\nSelect the observations corresponding to European countries in the year 1997;\nselect the variables related to gdp and country;\norder the countries by gdp.\n\n\ngapminder |&gt;\n  filter(continent == \"Europe\") |&gt;\n  filter(year == 1997) |&gt;\n  dplyr::select(country, gdpPercap) |&gt;\n  arrange(desc(gdpPercap)) |&gt;\n  head(n=5)\n\n# A tibble: 5 × 2\n  country     gdpPercap\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Norway         41283.\n2 Switzerland    32135.\n3 Netherlands    30246.\n4 Denmark        29804.\n5 Austria        29096.\n\n\n\n\n2.2.6.2 Total GDP of Japan in 1962\nTo answer this question we need to:\n\nSelect the observations corresponding to Japan in the year 1962;\nmultiply per capita GDP by total population.\n\n\ngapminder |&gt;\n  filter(country == \"Japan\" & year == 1962) |&gt;\n  mutate(totalGDP = gdpPercap * pop) |&gt;\n  pull(totalGDP)\n\n[1] 630251873021\n\n\n\n\n2.2.6.3 Average life expectancy for each continent in 2007\nTo answer this question we need to:\n\nSelect the observations corresponding to the year 2007;\ngroup the data by continent;\ncompute the average.\n\n\ngapminder |&gt;\n  filter(year == 2007) |&gt;\n  group_by(continent) |&gt;\n  summarize(averageExp = mean(lifeExp))\n\n# A tibble: 5 × 2\n  continent averageExp\n  &lt;fct&gt;          &lt;dbl&gt;\n1 Africa          54.8\n2 Americas        73.6\n3 Asia            70.7\n4 Europe          77.6\n5 Oceania         80.7\n\n\n\n\n2.2.6.4 Life expectancy per continent 1962-1997\n\ngapminder |&gt;\n  filter(year &gt;= 1962 & year &lt;= 1997) |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp))\n\n# A tibble: 40 × 3\n# Groups:   continent [5]\n   continent  year averageExp\n   &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n 1 Africa     1962       43.3\n 2 Africa     1967       45.3\n 3 Africa     1972       47.5\n 4 Africa     1977       49.6\n 5 Africa     1982       51.6\n 6 Africa     1987       53.3\n 7 Africa     1992       53.6\n 8 Africa     1997       53.6\n 9 Americas   1962       58.4\n10 Americas   1967       60.4\n# ℹ 30 more rows\n\n\nThis is too much information to see on the screen. We will see in the next session how to visualize the data in a plot.\nSome other useful functions defined in the dplyr package are the following.\n\nslice: select rows by position.\ncase_when and ifelse for conditional operators.\ntransmute: a mutate that drops existing variables.\ndo: perform arbitrary operations.\n\n\n\n\n\n\n\nExercise\n\n\n\nUse the dplyr verbs to answer the following questions.\n\nWhat were the top 5 most populous countries in Asia in 1952? And in 2007?\nWhat is the average GDP of European countries in 1982?\nWhat was the minimum and maximum population of Italy across the study years?\n[advanced] What is the average increase in per capita GDP per continent between 1952 and 2007?\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\n## 1\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder |&gt;\n    filter(year == 1952 & continent == \"Asia\") |&gt;\n    select(country, gdpPercap) |&gt;\n    arrange(desc(gdpPercap)) |&gt;\n    head(n=5)\n\ngapminder |&gt;\n    filter(year == 2007 & continent == \"Asia\") |&gt;\n    select(country, gdpPercap) |&gt;\n    arrange(desc(gdpPercap)) |&gt;\n    head(n=5)\n\n## 2\ngapminder |&gt;\n    filter(year == 1982 & continent == \"Europe\") |&gt;\n    summarize(averageGdp = mean(gdpPercap))\n\n## 3\ngapminder |&gt;\n    filter(country == \"Italy\") |&gt;\n    summarize(minPop = min(pop), maxPop = max(pop))\n\n## 4\ngapminder |&gt;\n    mutate(gdp1952 = ifelse(year == 1952, gdpPercap, NA),\n           gdp2007 = ifelse(year == 2007, gdpPercap, NA)) |&gt;\n    group_by(country, continent) |&gt;\n    summarize(gdpIncrease = sum(gdp2007, na.rm = TRUE) - sum(gdp1952, na.rm = TRUE)) |&gt;\n    group_by(continent) |&gt;\n    summarize(avgIncrease = mean(gdpIncrease))\n\n\n\n\n\n\n\n2.2.7 Plotting tidy data: the ggplot2 package\nAlthough base R has a plotting system, there are addon packages that define other plotting functions. One of them is the ggplot2 package. ggplot2 defines a “grammar of graphics” as a consistent way to create very different plots.\nIn my real analysis I use a mix of base graphics and ggplot2 as I find that both systems have pros and cons and each is more appropriate for certain types of data and/or plots. Here, we assume that you are already familiar with base graphics, or that you can learn it on your own, and we focus on ggplot2.\nLet’s start from the last example of the previous section. We want to explore the trends in life expectancy for each continent over the years. We can use group_by and summarize to compute the average life expectancy per each continent between 1962 and 2007. But how can we visualize it? Here is where the ggplot2 package comes into play.\n\ngapminder |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp)) |&gt;\n  ggplot(aes(x = year, y = averageExp, group=continent,\n             color=continent)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThe ggplot function has two main arguments:\n\na dataset that contains the data (in the example above passed with the pipe operator);\nan “aesthetic mapping” created by the aes() function.\n\nNote that in addition to specifying the x and y axis, we also specified a grouping variable and a variable that defines the color of the lines.\nHere, we plotted the data for the continent in the same plot with different colors. This is fine when plotting a few lines, but what if we wanted to plot the data for each country rather than each continent? A simple modification to our code let us do just that.\n\ntheme_set(theme_minimal())\n\ngapminder |&gt;\n  ggplot(aes(x = year, y = lifeExp, group=country,\n             color=continent)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis is not great because of overplotting. In this situations, a popular device is the use of facets.\n\ngapminder |&gt;\n  ggplot(aes(x = year, y = lifeExp, group=country,\n             color=continent)) +\n  geom_line() +\n  facet_wrap(~continent, nrow = 2)\n\n\n\n\n\n\n\n\nThis plot can be further improved by plotting only the average curve and the range for each continent.\n\ngapminder |&gt;\n  group_by(continent, year) |&gt;\n  summarize(averageExp = mean(lifeExp), \n            minExp = min(lifeExp),\n            maxExp = max(lifeExp)) |&gt;\n  ggplot(aes(x = year, y = averageExp, group=continent,\n             fill=continent)) +\n  geom_ribbon(aes(ymin = minExp, ymax = maxExp, alpha = 0.5)) +\n  geom_line() + theme(legend.position=\"none\") +\n  facet_wrap(~continent, nrow = 2)\n\n\n\n\n\n\n\n\nLet’s say that we are interested in the relation between life expectancy and GDP. Let’s have a look at the first available year, 1962. The best way to visualize the data to answer this question is with a scatterplot, available in ggplot2 via the geom_point.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis is not a very satisfying plot. The data looks better if we plot the x axis on the log scale.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() + scale_x_log10()\n\n\n\n\n\n\n\n\nTo extract more information from this analysis, we can also color the points by continent and have the size of the point proportional to the country’s population.\n\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp, color=continent, size=pop)) +\n  geom_point() + scale_x_log10()\n\n\n\n\n\n\n\n\nAnalogously, we can change the shape and transparency of the points with the parameters shape and alpha.\nHere we have seen three geometries, the line, the point, and the “ribbon”. Other useful geometries are geom_boxplot, geom_histogram, geom_density, etc. See the cheat sheet for all the geometries.\nThe last version of the plot is very close to an optimal representation of the data. This graphical summary raises some interesting questions, e.g., what is the country that sits by itself on the right-hand side of the plot? This is by far the richest country per capita, but its life expectancy is not very high.\nOne solution is to add labels.\n\nlibrary(ggrepel)\ngapminder |&gt;\n  filter(year == 1962) |&gt;\n  ggplot(aes(x = gdpPercap, y = lifeExp, color=continent, label=country)) +\n    geom_point(aes(size = pop)) +\n    geom_label_repel() + scale_x_log10()\n\n\n\n\n\n\n\n\nThis is a great plot for 1962, but how do things change across the years? We could use facets to display a few years, but if we are going to explore all years, we need a dynamic plot.\n\nlibrary(gganimate)\ngap &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +\n  geom_point(alpha = 0.7, show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  scale_x_log10() +\n  facet_wrap(~continent, ncol = 5) +\n  # animate it!\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  theme_bw()\n\ngap\n\n\n\n\n\n\n\n\nFinally, if we want to visualize the distribution of life expectancy, we can do that by using histograms, boxplots, violin plots or ridgeline plots.\n\nggplot(gapminder, aes(x=lifeExp)) +\n    geom_histogram(fill = \"dodgerblue\")\n\n\n\n\n\n\n\nggplot(gapminder, aes(x=continent, y=lifeExp, fill=continent)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nggplot(gapminder, aes(x=continent, y=lifeExp, fill=continent)) +\n    geom_violin()\n\n\n\n\n\n\n\nlibrary(ggridges)\nggplot(gapminder, aes(y=continent, x=lifeExp, fill=continent)) +\n    geom_density_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse ggplot to produce the following graphs.\n\nVisualize the distribution of life expectancy as a histogram + a density line (hint: geom_density, after_stat(density)).\nUse facets and the geometry of your choice to visualize the distribution of life expectancy for each continent and each year.\nReproduce the ridgeline plot above, changing the colors to be those stored in the continent_colors variable.\nCreate a scatterplot of life expectancy vs. GDP per capita across all years and continents, using 2d density (geom_density_2d) or hexagonal binning (geom_hex) to avoid overplotting.\nCreate a scatterplotof life expectancy vs. GDP per capita in 2007, with the point size proportional to population, color-coded by country, faceted by continent, adding text labels (hint: remove legend with theme(legend.position = \"none\")).\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\n## 1\nlibrary(tidyverse)\nlibrary(gapminder)\ngapminder |&gt;\n    ggplot(aes(x = lifeExp, y = after_stat(density))) +\n    geom_histogram() +\n    geom_density(linewidth = 2, col=\"red\")\n\n## 2\ngapminder |&gt;\n    ggplot(aes(x=factor(year), y=lifeExp)) +\n    geom_violin() +\n    facet_wrap(~continent)\n\n## 3\nlibrary(ggridges)\nggplot(gapminder, aes(y=continent, x=lifeExp, fill=continent)) +\n    geom_density_ridges() +\n    scale_fill_manual(values = continent_colors)\n\n## 4\ngapminder |&gt;\n    ggplot(aes(x = gdpPercap, y = lifeExp)) +\n    geom_hex() +\n    scale_x_log10()\n\n## 5\nlibrary(ggrepel)\ngapminder |&gt;\n    filter(year == 2007) |&gt;\n    ggplot(aes(x = gdpPercap, y = lifeExp, size = pop, col=country)) +\n    geom_point() +\n    scale_x_log10() +\n    theme(legend.position = \"none\") +\n    scale_color_manual(values = country_colors) +\n    geom_label_repel(aes(label = country), max.overlaps = 50) +\n    facet_wrap(~continent)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz.html#further-reading",
    "href": "dataviz.html#further-reading",
    "title": "2  Data Wrangling and Visualization",
    "section": "2.3 Further reading",
    "text": "2.3 Further reading\n\nClaus O. Wilke. Fundamentals of Data Visualization\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 3\nCarrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng Tidyverse Skills for Data Science",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling and Visualization</span>"
    ]
  },
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "",
    "text": "3.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#lab-tidy-data-in-r",
    "href": "dataviz2.html#lab-tidy-data-in-r",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.2 Lab: tidy data in R",
    "text": "3.2 Lab: tidy data in R\n\n3.2.1 What’s not covered (i.e., prerequisites)\nBasic R syntax is not covered in this lab, as we assume that you are already familiar with it. Most of the concepts in this first lab should be accessible to peolple with minimal exposure to R (Googling what you don’t remember is allowed – and encouraged!).\nIf you need help getting started with R, this is a good and free tutorial: https://swcarpentry.github.io/r-novice-gapminder/\n\n\n3.2.2 What’s covered (i.e., outline)\nIn this second lab, we will cover how to:\n\nimport the data into R\nclean the data and transform them into tidy data\nwork with more complex data structures\n\n\n\n3.2.3 Import and export the data\nThere are several functions to import the data into R depending on the format in which the data live outside of R. First, we will assume that the data exist in a standard text format, such as tab-delimited or comma-separated. We will briefly mention some packages that can be used to import data in MS Excel, Matlab, and other non-standard formats.\nBase R has many useful functions for the import of flat data tables, read.table and read.cvs among others. However, we will use the readr package here and in particular we will illustrate the read_csv, read_tsv and read_delim functions. These functions are faster and offer more flexibility compared to the base R counterparts. However, sometimes using read.table can be useful, for instance when one wants to use row.names.\n\n3.2.3.1 Read the data into R\nOften the data come in the form of csv files (comma-separated values). Excel spreadsheet can also be saved to csv files, making it a useful format to read into R data generated with Excel.\nAnother popular format is tsv files (tab-separated values). This is for instance how the gapminder data has been saved to file in the gapminder package.\nFinally, read_delim can be used to read files whose fields are separated by any character (most commonly a space) which need to be specified in the delim argument.\nHere, we will illustrate the use of read_tsv since the other two functions are very similar.\nFirst, we need a string that describes the location on disk of the file that we want to import in R. Note that the string will change depending on your OS.\n\nlibrary(readr)\ngap_tsv &lt;- system.file(\"extdata/gapminder.tsv\", package = \"gapminder\")\ngap_tsv\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/gapminder/extdata/gapminder.tsv\"\n\ngap &lt;- read_tsv(gap_tsv)\n\nRows: 1704 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): country, continent\ndbl (4): year, lifeExp, pop, gdpPercap\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngap\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nNote how the read_tsv function printed a message with the type that it inferred from each of the columns.\n\n\n3.2.3.2 Write the data to file\nSimilarly to read_tsv and read_csv, the functions write_tsv and write_csv can be used to write the R tables to a file on disk.\nNote that writing flat tables to csv or tsv files is the easiest and most reproducible approach. For complex data structures, say an histogram or a clustering tree, one can use the function save to save a binary R representation of the object on a .rda file. Similarly, the function load will read .rda files back into R.\n\n\n3.2.3.3 Additional resources for data import/export\nSometimes you may need to import data from different formats, e.g., because your collaborator has the data in a MS Excel file or because you are continuing in R an analysis started with SPSS, Stata, or SAS.\nFortunately, R has many packages that can be used for data import/export:\n\nhaven can be used to read SPSS, Stata, and SAS files.\nreadxl to read excel files (both .xls and .xlsx).\nDBI to import data from a variety of databases (advanced).\njsonlite to import json files\nxml2 to import XML files.\n\n\n\n\n3.2.4 Tidy the data: the tidyr package\nOften, a large amount of time is spent on “cleaning” the data, in what is sometimes referred to data wrangling.\nThis is because often the data are plagued by missing or implausible values, either because of technical issues with the data collection or because of human error. Moreover, data are often recorded in a way that is useful for storing them, but not ideal for analyzing them.\nThe objective of data wrangling is to take a messy dataset and make it tidy, so that it becomes easier to work with.\n\n3.2.4.1 What are tidy data?\nThe concept of tidy data was introduced by Hadley Wickham in his seminal paper.\nThere are three fundamental rules which make a dataset tidy:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nThe advantage of tidy data is both conceptual and practical: it allows you to have consistency between datasets and to use tools designed for this data structure. Moreover, because R works extremely well with vectorized operation it is very efficient to operate on the columns of a data frame.\n\n\n3.2.4.2 Examples of tidy and untidy data\nTo illustrate the concept of tidy data, we can use the examples in the EDAWR package. First we need to download and install it from Github with the devtools package.\n\nlibrary(remotes)\ninstall_github(\"rstudio/EDAWR\")\n\n\nstormscasespollution\n\n\n\nlibrary(EDAWR)\nstorms\n\n    storm wind pressure       date\n1 Alberto  110     1007 2000-08-03\n2    Alex   45     1009 1998-07-27\n3 Allison   65     1005 1995-06-03\n4     Ana   40     1013 1997-06-30\n5  Arlene   50     1010 1999-06-11\n6  Arthur   45     1010 1996-06-17\n\n\n\n\n\ncases\n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\n\n\n\n\npollution\n\n      city  size amount\n1 New York large     23\n2 New York small     14\n3   London large     22\n4   London small     16\n5  Beijing large    121\n6  Beijing small     56\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTake some time to list and describe the variables present in each datasets. Which of these datasets is tidy?\n\n\nNote that somebody’s tidy data is someone else’s untidy data. For instance in the pollution example, one could argue that the two variables in addition to city are: particle size, and particle amount. In this case the dataset is tidy. But another argument is that the variables are: amount of small particles and amount of large particles. In this case the dataset is untidy.\nFor this reason, I actually prefer another terminology: long data and wide data. The pollution dataset is stored in long format, while the cases dataset is stored in wide format.\nTo switch between long and wide format, the tidyr package provide two extremely useful functions: piveot_longer() and pivot_wider().\n\n\n3.2.4.3 Make your data “tall”: pivot_longer()\nThe function pivot_longer() can be used to transform the data from wide to tall.\nThe cases data frame is in wide form. If we want to make it tall, we can use the following command.\n\nlibrary(tidyr)\npivot_longer(cases, names_to = \"year\", values_to = \"n\", cols = 2:4)\n\n# A tibble: 9 × 3\n  country year      n\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n1 FR      2011   7000\n2 FR      2012   6900\n3 FR      2013   7000\n4 DE      2011   5800\n5 DE      2012   6000\n6 DE      2013   6200\n7 US      2011  15000\n8 US      2012  14000\n9 US      2013  13000\n\n\nThe names_to and values_to arguments are simply the names of the new columns that will have the variable values. The cols argument specify the columns that contain the data and that should be transformed.\n\n\n3.2.4.4 Make your data “wide”: pivot_wider()\nAnalogously, the pivot_wider() function let us go back to wide data from tall data.\nIf we pivot_wider a table that we previously pivot_longered, we should return to the original data representation.\n\ncases\n\n  country  2011  2012  2013\n1      FR  7000  6900  7000\n2      DE  5800  6000  6200\n3      US 15000 14000 13000\n\npivot_longer(cases, names_to = \"year\", values_to = \"n\", cols = 2:4)  |&gt;\n  pivot_wider(names_from = \"year\", values_from = \"n\")\n\n# A tibble: 3 × 4\n  country `2011` `2012` `2013`\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 FR        7000   6900   7000\n2 DE        5800   6000   6200\n3 US       15000  14000  13000\n\n\nAs we mentioned, sometimes the wide format is the tidy format. This could be the case for the pollution data, which we can transform in the following way.\n\npivot_wider(pollution, names_from = \"size\", values_from = \"amount\")\n\n# A tibble: 3 × 3\n  city     large small\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 New York    23    14\n2 London      22    16\n3 Beijing    121    56\n\n\nAdditional functions useful to transform the data are the separate() and unite() functions. You can find out what they do as an exercize.\n\n\n\n\n\n\nExercise\n\n\n\nExplore the dataset airquality:\n\nWhat are the variables? What are the observations?\nTransform it into a taller dataset, so that it will look like this:\n\n\n\n# A tibble: 612 × 4\n   Month   Day variable value\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     5     1 Ozone     41  \n 2     5     1 Solar.R  190  \n 3     5     1 Wind       7.4\n 4     5     1 Temp      67  \n 5     5     2 Ozone     36  \n 6     5     2 Solar.R  118  \n 7     5     2 Wind       8  \n 8     5     2 Temp      72  \n 9     5     3 Ozone     12  \n10     5     3 Solar.R  149  \n# ℹ 602 more rows\n\n\n\nStarting from the created dataset use pivot_wider to turn it back to the original\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\nair_long &lt;- airquality |&gt;\n    pivot_longer(c(Ozone, Solar.R, Wind, Temp), \n                 names_to = \"variable\", \n                 values_to = \"value\")\n\nair_wide &lt;- air_long |&gt;\n    pivot_wider(names_from = \"variable\", values_from = \"value\")\n\n\n\n\n\n\n3.2.4.5 Joining tables\nIn the tidy data framework, each table has one row per observational unit and one column per variable. But what if our analysis involves multiple observational unit types? In that case, there might be one table per observational unit types and you might need to join tables to perform certain data analyses.\nThe dplyr package contains several join functions. See ?inner_join for a full list. Here, we will cover only the inner and full joins, with the other operations left for homework exercises.\nTo illustrate joins, we will use the songs and artists datasets available in the EDAWR package. As you can see the observational units are quite different in the two tables, but nonetheless we might need both sets of variables in a single analysis.\n\nsongs\n\n                 song  name\n1 Across the Universe  John\n2       Come Together  John\n3      Hello, Goodbye  Paul\n4           Peggy Sue Buddy\n\nartists\n\n    name  plays\n1 George  sitar\n2   John guitar\n3   Paul   bass\n4  Ringo  drums\n\n\nThe inner join only returns those observations that are present in both datasets. You can think of it as a “intersection.”\n\nlibrary(dplyr)\ninner_join(songs, artists, by = \"name\")\n\n                 song name  plays\n1 Across the Universe John guitar\n2       Come Together John guitar\n3      Hello, Goodbye Paul   bass\n\n\nCoversely, the full join returns all observations that are present in either dataset. You can think of it as a “union.”\n\nfull_join(songs, artists, by = \"name\")\n\n                 song   name  plays\n1 Across the Universe   John guitar\n2       Come Together   John guitar\n3      Hello, Goodbye   Paul   bass\n4           Peggy Sue  Buddy   &lt;NA&gt;\n5                &lt;NA&gt; George  sitar\n6                &lt;NA&gt;  Ringo  drums\n\n\nFinally, the left and right join keep the left or right observations, respectively.\n\nleft_join(songs, artists)\n\n                 song  name  plays\n1 Across the Universe  John guitar\n2       Come Together  John guitar\n3      Hello, Goodbye  Paul   bass\n4           Peggy Sue Buddy   &lt;NA&gt;\n\nright_join(songs, artists)\n\n                 song   name  plays\n1 Across the Universe   John guitar\n2       Come Together   John guitar\n3      Hello, Goodbye   Paul   bass\n4                &lt;NA&gt; George  sitar\n5                &lt;NA&gt;  Ringo  drums\n\n\nNote how we can imply the “by” argument if we want to use all the common variable names between the two tables.\n\n\n\n\n\n\nExercise\n\n\n\nLoad the nycflights13 package to explore the four datasets flights, airlines, weather, and planes.\n\nJoin the tables flights and airlines in a way that each flight contains the names of the airline\nJoin the resulting table with the weather information\nJoin the resulting table with the airplane information\nUse tidyverse functions to extract the complete information (including weather and airplane info) of UA1545 flight departed on Jan 1st 2013.\nPlot departure delay versus wind speed: is there a relation between these variables? (Hint: avoid overplotting)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\n## 1-3\nlibrary(nycflights13)\nleft_join(flights, airlines) |&gt;\n    left_join(weather) |&gt;\n    left_join(planes) -&gt; complete_data\n\n## 4\ncomplete_data |&gt;\n    filter(year == 2013 & month == 1 & day == 1 & carrier == \"UA\", flight == 1545)\n\n## 5\ncomplete_data |&gt;\n    ggplot(aes(y = dep_delay, x = wind_speed)) +\n    geom_hex()\n\n\n\n\n\n\n\n3.2.5 The “tidyverse” vs. “base R”\nAs you look at examples or find answers to your questions online, you will notice that people refer to the set of packages that includes dplyr, tidyr, and ggplot2 as the tidyverse. You will often see people asking for a “tidyverse” way of doing something or for a “base R” way of doing something.\nThere is no right or wrong way to achieve something in R, but considerations about code readability and efficiency may be important depending on the application.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#sec-homework1",
    "href": "dataviz2.html#sec-homework1",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.3 Homework 1",
    "text": "3.3 Homework 1\nHealth policy in the United States is complicated, and several forms of healthcare coverage exist, including both coverage by federal goverment-led healthcare policy, and by private insurance companies. Before making any inference about the relationship between health condition and health policy, it is important for us to have a general idea about healthcare economics in the United States. Thus, we are interested in getting sense of healthcare coverage and healthcare spending across States. More specifically, the questions are:\n\nIs there a relationship between healthcare coverage and healthcare spending in the United States?\nHow does the spending distribution change across geographic regions in the United States?\nDoes the relationship between healthcare coverage and healthcare spending in the United States change from 2013 to 2014?\n\n\n3.3.1 Guided solution\n\nRead-in the data: use the read_csv function (readr package) to read the healthcare-coverage.cvs and the healthcare-spending.csv files (found here at data/KFF).\nLoad the state information found in the state datasets (datasets package). Note that you need to manually add information about the District of Columbia.\nAdd the abbreviation and the region of each state to the coverage dataset.\nJoin the coverage and spending datasets.\nUse ggplot to produce a scatterplot of the proportion of coverage by state vs. the spending per capita. Color code the points by region and add state abbreviations as labels. Use facet to stratify the analysis by type of coverage.\nUse ggplot to create a boxplot of spending per capita stratified by region.\nRepeat the graph in point 5 but faceting by year, in addition to coverage type. Hint: use the facet_grid() function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#further-reading",
    "href": "dataviz2.html#further-reading",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "3.4 Further reading",
    "text": "3.4 Further reading\n\nClaus O. Wilke. Fundamentals of Data Visualization\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 3\nCarrie Wright, Shannon E. Ellis, Stephanie C. Hicks and Roger D. Peng Tidyverse Skills for Data Science",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "dataviz2.html#footnotes",
    "href": "dataviz2.html#footnotes",
    "title": "3  Data Wrangling and Visualization (part 2)",
    "section": "",
    "text": "This homework is one of the Open Case Studies. Please try to solve this yourself before looking at the solution there.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling and Visualization (part 2)</span>"
    ]
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "4  Tools for reproducible research",
    "section": "",
    "text": "4.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tools for reproducible research</span>"
    ]
  },
  {
    "objectID": "git.html#introduction",
    "href": "git.html#introduction",
    "title": "4  Tools for reproducible research",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nThis lab is somewhat different from the others. In fact, I do not have written instructions, but it will be a live demonstration of the tools used to achieve reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tools for reproducible research</span>"
    ]
  },
  {
    "objectID": "git.html#quarto",
    "href": "git.html#quarto",
    "title": "4  Tools for reproducible research",
    "section": "4.3 Quarto",
    "text": "4.3 Quarto\nThis website, and all the slides of this course, are written in Quarto.\nQuarto is a literate programming language (Knuth 1984) that can be used to author documents, slides, websites and books. It works with R and Python using knitr or Jupyter as engines.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tools for reproducible research</span>"
    ]
  },
  {
    "objectID": "git.html#git-and-github",
    "href": "git.html#git-and-github",
    "title": "4  Tools for reproducible research",
    "section": "4.4 Git and Github",
    "text": "4.4 Git and Github\nGit is a version control software. Github is an open-source website that allows you to freely host your git repositories.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tools for reproducible research</span>"
    ]
  },
  {
    "objectID": "git.html#homework",
    "href": "git.html#homework",
    "title": "4  Tools for reproducible research",
    "section": "4.5 Homework",
    "text": "4.5 Homework\n\nCreate a Quarto document with the homework described in Section 3.3.\nFork the drisso/asda-homework repository on your Github account.\nAdd your homework’s qmd file to the homework1 folder.\nCreate a pull request to the main repo to turn in your homework.\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Tools for reproducible research</span>"
    ]
  },
  {
    "objectID": "linmod.html",
    "href": "linmod.html",
    "title": "5  Linear models",
    "section": "",
    "text": "5.1 Lecture Slides",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#lecture-slides",
    "href": "linmod.html#lecture-slides",
    "title": "5  Linear models",
    "section": "",
    "text": "5.1.1 Statistical modeling\n    View slides in full screen\n       \n      \n    \n  \n\n\n5.1.2 Linear models\n    View slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#lab-simple-linear-regression",
    "href": "linmod.html#lab-simple-linear-regression",
    "title": "5  Linear models",
    "section": "5.2 Lab: simple linear regression",
    "text": "5.2 Lab: simple linear regression\n\n5.2.1 Breast cancer dataset 1\nWe use a subset of the data from Sotiriou et al. (2006).\nThe data consist of 32 breast cancer patients with estrogen receptor positive tumors that underwent tamoxifen chemotherapy.\nThe variables available in the dataset are:\n\ngrade: histological grade of tumor (grade 1 vs 3);\nnode: lymph node status (0: not affected, 1: lymph nodes affected and removed);\nsize: tumor size in cm;\nage: patient’s age in years;\nESR1 and S100A8 gene expression in tumor biopsy (microarray technology).\n\nRead the data and glimpse at the data.\n\nlibrary(tidyverse)\ntheme_set(theme_minimal(base_size = 20))\n\nbrca &lt;- read_csv(\"slides/data/breastcancer.csv\")\nglimpse(brca)\n\nRows: 32\nColumns: 10\n$ sample_name &lt;chr&gt; \"OXFT_209\", \"OXFT_1769\", \"OXFT_2093\", \"OXFT_1770\", \"OXFT_1…\n$ filename    &lt;chr&gt; \"gsm65344.cel.gz\", \"gsm65345.cel.gz\", \"gsm65347.cel.gz\", \"…\n$ treatment   &lt;chr&gt; \"tamoxifen\", \"tamoxifen\", \"tamoxifen\", \"tamoxifen\", \"tamox…\n$ er          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ grade       &lt;dbl&gt; 3, 1, 1, 1, 3, 3, 1, 3, 1, 3, 3, 1, 3, 1, 1, 1, 1, 3, 1, 3…\n$ node        &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n$ size        &lt;dbl&gt; 2.5, 3.5, 2.2, 1.7, 2.5, 1.4, 3.3, 2.4, 1.7, 3.5, 1.4, 2.0…\n$ age         &lt;dbl&gt; 66, 86, 74, 69, 62, 63, 76, 61, 62, 65, 63, 70, 78, 71, 68…\n$ ESR1        &lt;dbl&gt; 1939.1990, 2751.9521, 379.1951, 2531.7473, 141.0508, 1495.…\n$ S100A8      &lt;dbl&gt; 207.19682, 36.98611, 2364.18306, 23.61504, 3218.74109, 107…\n\n\nThe goal is to study whether there is an association between the expression of ESR1 (Estrogen Receptor 1) and S100A8, a gene encoding for a calcium binding protein, involved in cell cycle progression and differentiation.\n\n\n5.2.2 Data wrangling and exploration\nFirst, we want to turn the categorical variables into factors.\n\nbrca &lt;- brca |&gt;\n    mutate(grade = factor(grade), node = factor(node), \n           er = factor(er), treatment = factor(treatment)) \nbrca\n\n# A tibble: 32 × 10\n   sample_name filename     treatment er    grade node   size   age  ESR1 S100A8\n   &lt;chr&gt;       &lt;chr&gt;        &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 OXFT_209    gsm65344.ce… tamoxifen 1     3     1       2.5    66 1939.  207. \n 2 OXFT_1769   gsm65345.ce… tamoxifen 1     1     1       3.5    86 2752.   37.0\n 3 OXFT_2093   gsm65347.ce… tamoxifen 1     1     1       2.2    74  379. 2364. \n 4 OXFT_1770   gsm65348.ce… tamoxifen 1     1     1       1.7    69 2532.   23.6\n 5 OXFT_1342   gsm65350.ce… tamoxifen 1     3     0       2.5    62  141. 3219. \n 6 OXFT_2338   gsm65352.ce… tamoxifen 1     3     1       1.4    63 1495.  108. \n 7 OXFT_2341   gsm65353.ce… tamoxifen 1     1     1       3.3    76 3406.   14.0\n 8 OXFT_1902   gsm65354.ce… tamoxifen 1     3     0       2.4    61 2813.   68.4\n 9 OXFT_1982   gsm65355.ce… tamoxifen 1     1     0       1.7    62  950.   74.2\n10 OXFT_5210   gsm65356.ce… tamoxifen 1     3     0       3.5    65 1053.  182. \n# ℹ 22 more rows\n\n\nWe can now look at the expression of the two genes across the patients.\n\nbrca |&gt; pivot_longer(cols = c(\"ESR1\", \"S100A8\"), names_to = \"gene\", values_to = \"expression\") -&gt; brca_long\n\nbrca_long |&gt;\n    ggplot(aes(gene, expression)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWould the distribution of the genes look better in log scale? Repeat the plot log-transforming the y axis\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\nbrca_long |&gt;\n    ggplot(aes(gene, expression)) +\n    geom_boxplot() +\n    scale_y_log10()\n\n\n\n\nLet’s now assess the relation between the two genes.\n\nbrca |&gt;\n    ggplot(aes(ESR1, S100A8)) +\n    geom_point()\n\n\n\n\n\n\n\n\nIt is clear that the scatterplot is influenced by three outliers in the S100A8 gene. We can try to remove them and repeat the plot.\n\nbrca |&gt;\n    filter(S100A8 &lt; 2000) -&gt; brca_outrm\n\nbrca_outrm |&gt;\n    ggplot(aes(ESR1, S100A8)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCan you think of a different way to improve the visualization that keeps the outliers in the graph?\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\nbrca |&gt;\n    ggplot(aes(ESR1, S100A8)) +\n    geom_point() +\n    scale_y_log10()\n\n\n\n\nIs linear regression a good model for this relationship? We can get a sense of this by fitting a line and a smooth curve to the plot above. We can achieve this by using the geom_smooth() geometry.\n\nbrca_outrm |&gt;\n    ggplot(aes(ESR1, S100A8)) +\n    geom_point() +\n    geom_smooth(se = FALSE, span=1) +\n    geom_smooth(method = lm, color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nThere may be a non-linear relation between the two genes, but the linear regression fits the points fairly well.\n\n\n5.2.3 Simple linear regression\nLet’s fit a linear model.\n\nfit &lt;- lm(S100A8 ~ ESR1, data=brca_outrm)\nsummary(fit)\n\n\nCall:\nlm(formula = S100A8 ~ ESR1, data = brca_outrm)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-95.43 -34.81  -6.79  34.23 145.21 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 208.47145   28.57207   7.296 7.56e-08 ***\nESR1         -0.05926    0.01212  -4.891 4.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 59.91 on 27 degrees of freedom\nMultiple R-squared:  0.4698,    Adjusted R-squared:  0.4502 \nF-statistic: 23.93 on 1 and 27 DF,  p-value: 4.078e-05\n\n\nWe can see that we have an \\(R^2\\) of 0.47, indicating that ESR1 helps explaining the expression of S100A8.\nIts coefficient is highly significant and our predicted values are \\[\n\\hat{y} = 208.5 - 0.06 x,\n\\] which means that for each unit increase in the expression of ESR1, S100A8’s expression decreases by 0.06 units.\n\n\n\n\n\n\nExercise\n\n\n\n\nWhat is the predicted S100A8 expression of a patient with ESR1 expression of 1000?\nWhat is the predicted S100A8 expression of a patient with ESR1 expression of 3000?\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\npredict(fit, newdata=data.frame(ESR1=1000))\nfit$coefficients[1] + fit$coefficients[2] * 1000\npredict(fit, newdata=data.frame(ESR1=3000))\nfit$coefficients[1] + fit$coefficients[2] * 3000\n\n\n\n\n\n5.2.3.1 And the outliers?\nWhat would have happened if we had not taken the outliers out of the analysis?\n\nfit_out &lt;- lm(S100A8 ~ ESR1, data=brca)\nsummary(fit_out)\n\n\nCall:\nlm(formula = S100A8 ~ ESR1, data = brca)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1237.8  -634.6  -191.5   251.4  4620.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1709.4637   409.9006   4.170 0.000239 ***\nESR1          -0.6373     0.1825  -3.493 0.001506 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1076 on 30 degrees of freedom\nMultiple R-squared:  0.2891,    Adjusted R-squared:  0.2654 \nF-statistic:  12.2 on 1 and 30 DF,  p-value: 0.001506\n\n\nWe see that the outliers have a huge impact on the analysis. The \\(R^2\\) significantly drops, the significance of the coefficient is much less (although it is still significantly different from 0), and its value is a lot further from 0.\nIn fact, this model indicates that for each unit increase in the expression of ESR1, S100A8’s expression decreases by 0.64 units.\n\n\n\n\n\n\nExercise\n\n\n\nAccording to this new model:\n\nWhat is the predicted S100A8 expression of a patient with ESR1 expression of 1000?\nWhat is the predicted S100A8 expression of a patient with ESR1 expression of 3000?\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\npredict(fit_out, newdata=data.frame(ESR1=1000))\nfit_out$coefficients[1] + fit_out$coefficients[2] * 1000\npredict(fit_out, newdata=data.frame(ESR1=3000))\nfit_out$coefficients[1] + fit_out$coefficients[2] * 3000\n\n\n\n\nWhich model is correct? Should we include the outliers or not? Is there an alternative?\n\n\n5.2.3.2 Transforming the predictor\nRemember that the linear model is linear in the coefficients, meaning that we can transform the scale of the variables if we think that this better reflects their nature. In fact, microarray technologies are based on fluorescence intensities, which typically show a skewed, positive distribution and may benefit from a log transformation.\n\nbrca |&gt;\n    ggplot(aes(log(ESR1), log(S100A8))) +\n    geom_point() +\n    geom_smooth(se = FALSE) +\n    geom_smooth(method = lm, color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nNow the relation looks a lot more linear than in the linear scale.\nWe can fit this new model.\n\nfit_log &lt;- lm(log(S100A8) ~ log(ESR1), data=brca)\nsummary(fit_log)\n\n\nCall:\nlm(formula = log(S100A8) ~ log(ESR1), data = brca)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.34664 -0.46120  0.05631  0.47458  1.33579 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   16.221      1.111   14.60 3.57e-15 ***\nlog(ESR1)     -1.615      0.150  -10.76 8.07e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7109 on 30 degrees of freedom\nMultiple R-squared:  0.7942,    Adjusted R-squared:  0.7874 \nF-statistic: 115.8 on 1 and 30 DF,  p-value: 8.07e-12\n\n\nWith this model we get a very high \\(R^2\\) of 0.79 and a highly significant association between the two genes.\nThe predicted values are now \\[\n\\widehat{\\log y} = 16.22 - 1.61 \\log x,\n\\] which means that for each unit increase in the log expression of ESR1, S100A8’s log expression decreases by 1.61 units.\nNote that a model that is additive in a log scale is multiplicative in the natural scale. As an example, let’s see what happens to two patients: \\[\n\\log \\hat{y}_1 = 16.22 - 1.61 \\log x_1 \\quad \\quad \\log \\hat{y}_2 = 16.22 - 1.61 \\log x_2,\n\\] hence \\[\n\\log \\hat{y}_2 - \\log \\hat{y}_1 = -1.61 (\\log \\hat{x}_2 - \\log \\hat{x}_1)\n\\] which means \\[\n\\log \\left(\\frac{\\hat{y}_2}{\\hat{y}_1}\\right) = -1.61  \\log \\left(\\frac{\\hat{x}_2}{\\hat{x}_1}\\right)\n\\] and in the linear scale \\[\n\\frac{\\hat{y}_2}{\\hat{y}_1} = \\left(\\frac{\\hat{x}_2}{\\hat{x}_1}\\right)^{-1.61}\n\\] If patient 2 has expression of S100A8 twice that of patient 1, we have that their expression of ESR1 is \\[\n\\hat{y}_2 = 2^{-1.61 } \\hat{y}_1 \\approx 0.33 \\hat{y}_1\n\\] i.e., approximately \\(33\\%\\) of that of patient 1 (or three times lower).\n\n\n\n5.2.4 How to choose a model\nWe have seen that different modeling choices lead to different results. The question of which model to choose is hence paramount.\nThere are several considerations to make for this choice.\nThe first, and perhaps most important, is checking the assumptions of our model. Remember that the main assumptions are:\n\nIndependence of the errors\nLinearity\nHomoscedasticity\nNormality of the errors (for inference)\n\nFor the linearity assumption, one can check the above plots; for this example, it seems that the log-scale is better.\nFor the other assumptions, we can perform what is known as the analysis of the residuals.\nLet’s start from the first model.\n\nplot(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis command produces four graphs:\n\nResiduals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary.\nNormal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required.\nScale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic.\nResiduals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model’s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots.\n\nIn this case, the residuals do not look bad, but there may be a small issue with linearity (first graph) and heteroschedasticity (third graph).\nThe second model seems slightly better on all analyses.\n\nplot(fit_log)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nRun the residuals analysis of the linear model without removing the outliers. How would you rate the model fit?\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\nplot(fit_out)\n\n\n\n\nUltimately in this case, we can choose the model with the log transformation, which seems better in terms of \\(R^2\\) and interpretation of the results.\n\n\n5.2.5 Predictions\nLet’s go back to parameter estimation. Here, we have estimated the two regression coefficients, we can also compute confidence intervals for those estimates.\n\nconfint(fit_log)\n\n                2.5 %    97.5 %\n(Intercept) 13.952114 18.489024\nlog(ESR1)   -1.921047 -1.308185\n\n\nThis allows us to be more confident on the inferential statements about the relation between the two genes.\nHowever, very often one of the goals of linear modeling is prediction. For instance, we can try and predict what would be the expression value of S100A8 for a patient that shows a value of 2000 for ESR1.\n\nlogpred &lt;- predict(fit_log, newdata = data.frame(ESR1 = 2000))\nexp(logpred)\n\n       1 \n51.83325 \n\n\nAlso in this case it would be more useful to have a prediction interval rather than a point prediction.\nWhen computing prediction intervals we have to “adjust” the confidence interval to take into consideration, in addition to the uncertainty of estimation of the model parameters, the uncertainty of the new observation, which is a stochastic quantity because it is yet to be observed.\n\ngrid &lt;- 140:4000\npred &lt;- predict(fit_log, newdata = data.frame(ESR1 = grid), interval = \"prediction\")\nhead(pred)\n\n       fit      lwr      upr\n1 8.241715 6.592193 9.891237\n2 8.230223 6.581678 9.878769\n3 8.218812 6.571235 9.866390\n4 8.207482 6.560862 9.854101\n5 8.196230 6.550560 9.841900\n6 8.185056 6.540327 9.829785\n\n\nWe can use these predictions to visually represent the prediction uncertainty of new data points.\n\nnewdata &lt;- data.frame(cbind(grid, exp(pred)))\nbrca |&gt; ggplot(aes(x = ESR1, y = S100A8)) +\n    geom_point() +\n    geom_line(aes(x = grid, y = fit), newdata) +\n    geom_line(aes(x = grid, y = lwr), newdata, color = \"grey\") +\n    geom_line(aes(x = grid, y = upr), newdata, color = \"grey\")\n\n\n\n\n\n\n\n\nNote that although the model was fit on the log-transformed variables, here we are visualizing it in the linear scale.\nWe can of course visualize the model in the log scale, and we will recognize its linear nature.\n\nbrca |&gt; ggplot(aes(x = ESR1, y = S100A8)) +\n    geom_point() +\n    geom_line(aes(x = grid, y = fit), newdata) +\n    geom_line(aes(x = grid, y = lwr), newdata, color = \"grey\") +\n    geom_line(aes(x = grid, y = upr), newdata, color = \"grey\") +\n    scale_y_log10() + scale_x_log10()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#lab-multiple-regression",
    "href": "linmod.html#lab-multiple-regression",
    "title": "5  Linear models",
    "section": "5.3 Lab: multiple regression",
    "text": "5.3 Lab: multiple regression\n\n5.3.1 Prostate cancer dataset 2\nFor this lab we use a dataset that contains measurements of Prostate specific antigen (PSA) and a number of clinical variables for 97 males with radical prostatectomy.\nPSA levels can help diagnose prostate cancer: a high level can be a sign of cancer.\nHere, we are interested in evaluating the association between PSA and the other available variables, i.e.:\n\ntumor volume (lcavol)\nprostate weight (lweight)\nage\nbenign prostate hypertrophy (lbph)\nseminal vesicle invasion (svi)\ncapsular penetration (lcp)\nGleason score (gleason)\npercentage Gleason score 4/5 (pgg45)\n\n\nprostate &lt;- read_csv(\"slides/data/prostate.csv\")\nglimpse(prostate)\n\nRows: 97\nColumns: 9\n$ lcavol  &lt;dbl&gt; -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -1.…\n$ lweight &lt;dbl&gt; 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, 3.…\n$ age     &lt;dbl&gt; 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, 66…\n$ lbph    &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ svi     &lt;chr&gt; \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"health…\n$ lcp     &lt;dbl&gt; -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ gleason &lt;dbl&gt; 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, 6,…\n$ pgg45   &lt;chr&gt; \"healthy\", \"healthy\", \"20\", \"healthy\", \"healthy\", \"healthy\", \"…\n$ lpsa    &lt;dbl&gt; -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…\n\n\nAs in the previous case, we want to transform categorical variables into factors.\n\nprostate &lt;- prostate |&gt;\n    mutate(svi = factor(svi), gleason = factor(gleason),\n           pgg45 = factor(pgg45))\nprostate\n\n# A tibble: 97 × 9\n   lcavol lweight   age   lbph svi       lcp gleason pgg45     lpsa\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;\n 1 -0.580    2.77    50 -1.39  healthy -1.39 6       healthy -0.431\n 2 -0.994    3.32    58 -1.39  healthy -1.39 6       healthy -0.163\n 3 -0.511    2.69    74 -1.39  healthy -1.39 7       20      -0.163\n 4 -1.20     3.28    58 -1.39  healthy -1.39 6       healthy -0.163\n 5  0.751    3.43    62 -1.39  healthy -1.39 6       healthy  0.372\n 6 -1.05     3.23    50 -1.39  healthy -1.39 6       healthy  0.765\n 7  0.737    3.47    64  0.615 healthy -1.39 6       healthy  0.765\n 8  0.693    3.54    58  1.54  healthy -1.39 6       healthy  0.854\n 9 -0.777    3.54    47 -1.39  healthy -1.39 6       healthy  1.05 \n10  0.223    3.24    63 -1.39  healthy -1.39 6       healthy  1.05 \n# ℹ 87 more rows\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExplore the relation between PSA and the other variables with a series of plots.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCode\nprostate |&gt;\n    ggplot(aes(y = lpsa, x = lcavol, color=svi)) +\n    geom_point()\n\nprostate |&gt;\n    ggplot(aes(y = lpsa, x = lcavol, color=factor(gleason))) +\n    geom_point()\n\nprostate |&gt;\n    ggplot(aes(y = lpsa, x = lweight, color=svi)) +\n    geom_point()\n\n\n\n\n\n\n5.3.2 Regression model\nWe can now fit a linear model, including some of the available variables.\n\nfit_pr &lt;- lm(lpsa ~ lcavol + lweight + svi + gleason, data=prostate)\nsummary(fit_pr)\n\n\nCall:\nlm(formula = lpsa ~ lcavol + lweight + svi + gleason, data = prostate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7456 -0.4344  0.0000  0.4979  1.5694 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.43053    0.55165  -0.780 0.437176    \nlcavol       0.50007    0.08064   6.202 1.66e-08 ***\nlweight      0.52053    0.15028   3.464 0.000817 ***\nsviinvasion  0.59241    0.21274   2.785 0.006531 ** \ngleason7     0.34034    0.17951   1.896 0.061178 .  \ngleason8    -0.02720    0.72707  -0.037 0.970242    \ngleason9     0.15597    0.36239   0.430 0.667943    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7137 on 90 degrees of freedom\nMultiple R-squared:  0.6416,    Adjusted R-squared:  0.6177 \nF-statistic: 26.85 on 6 and 90 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nExercise\n\n\n\nComment the output, with particular focus on \\(R^2\\), coefficient estimates and significance.\n\n\n\n\n5.3.3 Model selection\nIs the inclusion of the Gleason score informative or not? Which model is better, with or without such variable?\nTo test for the global significance of the gleason variable, we can fit a model without it and perform an Analysis of Variance (a.k.a. an F-test) to test the two models.\n\nfit_nogl &lt;- lm(lpsa ~ lcavol + lweight + svi, data=prostate)\nsummary(fit_nogl)\n\n\nCall:\nlm(formula = lpsa ~ lcavol + lweight + svi, data = prostate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.72966 -0.45767  0.02814  0.46404  1.57012 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.26807    0.54350  -0.493  0.62301    \nlcavol       0.55164    0.07467   7.388  6.3e-11 ***\nlweight      0.50854    0.15017   3.386  0.00104 ** \nsviinvasion  0.66616    0.20978   3.176  0.00203 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7168 on 93 degrees of freedom\nMultiple R-squared:  0.6264,    Adjusted R-squared:  0.6144 \nF-statistic: 51.99 on 3 and 93 DF,  p-value: &lt; 2.2e-16\n\nanova(fit_nogl, fit_pr)\n\nAnalysis of Variance Table\n\nModel 1: lpsa ~ lcavol + lweight + svi\nModel 2: lpsa ~ lcavol + lweight + svi + gleason\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     93 47.785                           \n2     90 45.845  3    1.9403 1.2697 0.2896\n\n\nThe residuals sum of squares will always decrease with the addition of a variable (and the \\(R^2\\) will always increase), but this does not mean that the new variable significantly adds new information in explaining/predicting the response.\nIn this case, we could decide to drop the gleason variable, but significance is not always the only consideration to make.\nModel selection is a very important (and difficult!) problem in statistics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#homework-2",
    "href": "linmod.html#homework-2",
    "title": "5  Linear models",
    "section": "5.4 Homework 3",
    "text": "5.4 Homework 3\nLittle et al. (2009) assessed the practical values of telemonitoring devices for remote symptom progression monitoring of early-stage Parkinson’s disease patients.\nThe collected dataset is composed of a range of biomedical voice measurements from 42 people with early-stage Parkinson’s disease recruited to a six-month trial of a telemonitoring device for remote symptom progression monitoring. The recordings were automatically captured in the patient’s homes.\nThe available variables are: - subject number, - subject age, - subject gender, - time interval from baseline recruitment date, - motor UPDRS (Unified Parkinson’s Disease Rating Scale), - total UPDRS (Unified Parkinson’s Disease Rating Scale), - and 16 biomedical voice measures.\nEach row corresponds to one of 5,875 voice recording from these individuals. The main aim of the data is to predict the motor and total UPDRS scores (‘motor_UPDRS’ and ‘total_UPDRS’) from the 16 voice measures.\nThe data are available here.\n\n5.4.1 Guided solution\n\nRead the original paper to familiarize yourself with the research and the data (an open-access version is available here). Pay special attention to the definition of the measurements.\nDownload and unzip the data folder.\nRead the data (parkinsons+telemonitoring/parkinsons_updrs.data) with the read_csv function from the readr package.\nExploratory Analysis:\n\nexplore the pattern of the 16 measures and the two UPDRS scores over time per patient\n\nPatient-level data:\n\nuse the group_by and summarize_all functions to compute the mean and standard deviation per patient of each of the 16 voice measures and the mean per patient of the two UPDRS scores.\n\nPatient-level EDA:\n\nexplore the correlation between the 16 biomedical voice measures\nexplore the correlation between each of them and the motor UPDRS score\n\nFit a linear model using total_UPDRS as a response variable, and as covariates\n\na subset of the 16 mean and variance measures that seem to be informative, and\nthe patients clinical information (e.g., age and sex)\n\n\nNote: be careful about collinearity or near-collinearity in the data, e.g., between different Jitter measurements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#further-reading",
    "href": "linmod.html#further-reading",
    "title": "5  Linear models",
    "section": "5.5 Further reading",
    "text": "5.5 Further reading\n\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 2\nR. A. Irizarry and M. I. Love. Data Analysis for the Life Sciences.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#references",
    "href": "linmod.html#references",
    "title": "5  Linear models",
    "section": "References",
    "text": "References\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLittle, Max A, Patrick E McSharry, Eric J Hunter, Jennifer Spielman, and\nLorraine O Ramig. 2009. “Suitability of Dysphonia Measurements for\nTelemonitoring of Parkinson’s Disease.” IEEE Transactions on\nBiomedical Engineering 56 (4): 1015–22.\n\n\nSotiriou, Christos, Pratyaksha Wirapati, Sherene Loi, Adrian Harris,\nSteve Fox, Johanna Smeds, Hans Nordgren, et al. 2006. “Gene\nExpression Profiling in Breast Cancer: Understanding the Molecular Basis\nof Histologic Grade to Improve Prognosis.” Journal of the\nNational Cancer Institute 98 (4): 262–72.\n\n\nWolff, Jonas O, and Stanislav N Gorb. 2013. “Radial Arrangement of\nJanus-Like Setae Permits Friction Control in Spiders.”\nScientific Reports 3 (1): 1101.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod.html#footnotes",
    "href": "linmod.html#footnotes",
    "title": "5  Linear models",
    "section": "",
    "text": "this example is from the Practical Statistics for the Life Sciences course↩︎\nthis example is from the Practical Statistics for the Life Sciences course↩︎\nThis homework is based on data from the UC Irvine Machine Learning Repository.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "linmod2.html",
    "href": "linmod2.html",
    "title": "6  Linear models (part 2)",
    "section": "",
    "text": "6.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod2.html#lab-confounding-and-adjustment-1",
    "href": "linmod2.html#lab-confounding-and-adjustment-1",
    "title": "6  Linear models (part 2)",
    "section": "6.2 Lab: confounding and adjustment 1",
    "text": "6.2 Lab: confounding and adjustment 1\nIn this Lab, we will use simulations to show the effect of modelling choices on inference.\nLet’s consider this rather general situation, in which a continuous outcome is influenced by a binary treatment \\(z\\) (that we are interested in studying) and a continuous confounder \\(x\\): \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 z_i + \\varepsilon_i.\n\\]\nNote that here we are using the outcome/treatment/confounder terminology, typical of epidemiology.\n\n6.2.1 Experiment 1\nLet’s simulate some data, starting from an ideal situation.\n\nset.seed(1503)\n\nn &lt;- 200\nz &lt;- rep(c(0, 1), each=n/2)\nx &lt;- runif(n)\nbeta0 &lt;- 0\nbeta1 &lt;- 2\nbeta2 &lt;- 1\nsigma &lt;- .2\n\ny &lt;- beta0 + beta1 * z + beta2 * x + rnorm(n, sd = sigma)\n\nLet’s plot the data.\n\ndf &lt;- data.frame(x, y, z=factor(z))\nggplot(df, aes(x = x, y = y, color = z)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nLet’s look at the marginal relation between \\(y\\) and \\(z\\).\n\nggplot(df, aes(y = y, x = z, fill=z)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n:::\nLet’s finally look at the relation between \\(x\\) and \\(z\\).\n\nggplot(df, aes(y = x, x = z, fill=z)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nThe marginal relation between \\(z\\) and \\(y\\) is consistent with the conditional relation. This is because \\(x\\) and \\(z\\) are independent.\nLet’s fit a linear model.\n\nfit1 &lt;- lm(y ~ z + x)\nsummary(fit1)\n\n\nCall:\nlm(formula = y ~ z + x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4650 -0.1437 -0.0031  0.1498  0.4484 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.02005    0.03073  -0.652    0.515    \nz            1.99099    0.02751  72.385   &lt;2e-16 ***\nx            1.04417    0.04888  21.362   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1945 on 197 degrees of freedom\nMultiple R-squared:  0.9668,    Adjusted R-squared:  0.9665 \nF-statistic:  2869 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see that the estimates are not far from the true estimated values.\n\ndata.frame(estimate = round(fit1$coefficients, 2),\n           true =c(beta0, beta1, beta2))\n\n            estimate true\n(Intercept)    -0.02    0\nz               1.99    2\nx               1.04    1\n\n\nWhat happens if we omit \\(x\\) from the model?\n\nfit1b &lt;- lm(y ~ z)\nsummary(fit1b)\n\n\nCall:\nlm(formula = y ~ z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78648 -0.26678  0.00481  0.26019  0.80658 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.48822    0.03533   13.82   &lt;2e-16 ***\nz            1.99867    0.04996   40.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3533 on 198 degrees of freedom\nMultiple R-squared:  0.8899,    Adjusted R-squared:  0.8894 \nF-statistic:  1600 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case the estimate of the coefficient for \\(z\\) is still good, since \\(x\\) does not confound the relation between \\(z\\) and \\(y\\).\n\ndata.frame(estimate = round(fit1b$coefficients, 2),\n           true =c(beta0, beta1))\n\n            estimate true\n(Intercept)     0.49    0\nz               2.00    2\n\n\n\n\n6.2.2 Experiment 2\nLet’s now include some dependence in the data generation, e.g., by estimating \\(x\\) differently for the two categories of \\(z\\).\n\nset.seed(1538)\n\nn &lt;- 200\nz &lt;- rep(c(0, 1), each=n/2)\nx &lt;- c(runif(n/2), runif(n/2, min = 1.5, max = 2.5))\nbeta0 &lt;- 0\nbeta1 &lt;- 2\nbeta2 &lt;- 1\nsigma &lt;- .2\n\ny &lt;- beta0 + beta1 * z + beta2 * x + rnorm(n, sd = sigma)\n\nLet’s plot the data.\n\ndf &lt;- data.frame(x, y, z=factor(z))\nggplot(df, aes(x = x, y = y, color = z)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nNow we can see that, as expected, \\(x\\) depends on \\(z\\).\n\nggplot(df, aes(y = x, x = z, fill=z)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nThis translates into a difference between the marginal and conditional relation between \\(z\\) and \\(y\\).\n\nggplot(df, aes(y = y, x = z, fill=z)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nNotice the “compound effect” of \\(x\\) and \\(z\\) on \\(y\\).\nLet’s fit the correct model.\n\nfit2 &lt;- lm(y ~ z + x)\nsummary(fit2)\n\n\nCall:\nlm(formula = y ~ z + x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45873 -0.14348  0.01642  0.12145  0.49850 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.03663    0.03300   -1.11    0.268    \nz            1.99659    0.07518   26.56   &lt;2e-16 ***\nx            1.02294    0.04944   20.69   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1988 on 197 degrees of freedom\nMultiple R-squared:  0.9874,    Adjusted R-squared:  0.9872 \nF-statistic:  7694 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nAs expected, we get good estimates of the coefficients.\n\ndata.frame(estimate=round(fit2$coefficients, 2), \n           true=round(c(beta0, beta1, beta2), 2))\n\n            estimate true\n(Intercept)    -0.04    0\nz               2.00    2\nx               1.02    1\n\n\nWhat happens if we now omit the \\(x\\) confounder?\n\nfit2b &lt;- lm(y ~ z)\nsummary(fit2b)\n\n\nCall:\nlm(formula = y ~ z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.68982 -0.25933  0.00184  0.25912  0.92836 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.50825    0.03533   14.39   &lt;2e-16 ***\nz            3.43924    0.04996   68.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3533 on 198 degrees of freedom\nMultiple R-squared:  0.9599,    Adjusted R-squared:  0.9597 \nF-statistic:  4739 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nWe are overestimating the treatment effect!\n\ndata.frame(estimate=round(fit2b$coefficients, 2), \n           true=c(beta0, beta1))\n\n            estimate true\n(Intercept)     0.51    0\nz               3.44    2\n\n\n\n\n6.2.3 Experiment 3\nThings get more complicated when we simulate coefficients with opposite signs.\n\nset.seed(1547)\n\nn &lt;- 200\nz &lt;- rep(c(0, 1), each=n/2)\nx &lt;- c(runif(n/2), runif(n/2, min = 1, max = 2))\nbeta0 &lt;- 0\nbeta1 &lt;- -1\nbeta2 &lt;- 1\nsigma &lt;- .2\n\ny &lt;- beta0 + beta1 * z + beta2 * x + rnorm(n, sd = sigma)\n\nLet’s plot the data.\n\ndf &lt;- data.frame(x, y, z=factor(z))\nggplot(df, aes(x = x, y = y, color = z)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nNow that the effects of two correlated variables are opposite to each other, the marginal effect of \\(z\\) on \\(y\\) becomes almost 0.\n\nggplot(df, aes(y = y, x = z, fill=z)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nLet’s fit a linear model.\n\nfit3 &lt;- lm(y ~ z + x)\nsummary(fit3)\n\n\nCall:\nlm(formula = y ~ z + x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48522 -0.15172 -0.00461  0.14135  0.54230 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01783    0.03188  -0.559    0.577    \nz           -0.98838    0.05627 -17.566   &lt;2e-16 ***\nx            1.00277    0.04980  20.138   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.193 on 197 degrees of freedom\nMultiple R-squared:  0.673, Adjusted R-squared:  0.6697 \nF-statistic: 202.8 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the correct model correctly estimates the parameters.\n\ndata.frame(estimate=round(fit3$coefficients, 2), \n           true=c(beta0, beta1, beta2))\n\n            estimate true\n(Intercept)    -0.02    0\nz              -0.99   -1\nx               1.00    1\n\n\nBut when omitting \\(x\\), we lose the significant association between \\(z\\) and \\(y\\).\n\nfit3b &lt;- lm(y ~ z)\nsummary(fit3b)\n\n\nCall:\nlm(formula = y ~ z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.80393 -0.25652  0.04517  0.22719  0.84438 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.493279   0.033660  14.655   &lt;2e-16 ***\nz           0.002515   0.047602   0.053    0.958    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3366 on 198 degrees of freedom\nMultiple R-squared:  1.41e-05,  Adjusted R-squared:  -0.005036 \nF-statistic: 0.002792 on 1 and 198 DF,  p-value: 0.9579\n\n\nIn fact we are overestimating the treatment effect to essentially 0!\n\ndata.frame(estimate=round(fit3b$coefficients, 2), \n           true=c(beta0, beta1))\n\n            estimate true\n(Intercept)     0.49    0\nz               0.00   -1\n\n\n\n\n6.2.4 Experiment 4\nEven more dramatic effects can be observed when the variable of interest is \\(x\\).\n\nset.seed(1547)\n\nn &lt;- 200\nz &lt;- rep(c(0, 1), each=n/2)\nx &lt;- c(runif(n/2), runif(n/2, min = 1, max = 2))\nbeta0 &lt;- 0\nbeta1 &lt;- -2\nbeta2 &lt;- 1\nsigma &lt;- .2\n\ny &lt;- beta0 + beta1 * z + beta2 * x + rnorm(n, sd = sigma)\n\nLet’s plot the data.\n\ndf &lt;- data.frame(x, y, z=factor(z))\nggplot(df, aes(x = x, y = y, color = z)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nIn this case, while the conditional effect of \\(x\\) on \\(y\\) is positive, its marginal effect is negative!\nLet’s fit a linear model.\n\nfit4 &lt;- lm(y ~ z + x)\nsummary(fit4)\n\n\nCall:\nlm(formula = y ~ z + x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.48522 -0.15172 -0.00461  0.14135  0.54230 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01783    0.03188  -0.559    0.577    \nz           -1.98838    0.05627 -35.339   &lt;2e-16 ***\nx            1.00277    0.04980  20.138   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.193 on 197 degrees of freedom\nMultiple R-squared:  0.8984,    Adjusted R-squared:  0.8974 \nF-statistic: 870.9 on 2 and 197 DF,  p-value: &lt; 2.2e-16\n\n\nOnce more, the correct model correctly estimates the parameters.\n\ndata.frame(estimate=round(fit4$coefficients, 2), \n           true=c(beta0, beta1, beta2))\n\n            estimate true\n(Intercept)    -0.02    0\nz              -1.99   -2\nx               1.00    1\n\n\nBut when omitting \\(z\\), we invert the sign of the \\(x\\) coefficient!\n\nfit4b &lt;- lm(y ~ x)\nsummary(fit4b)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28221 -0.28897  0.04716  0.35760  1.31902 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.53271    0.07517   7.087 2.36e-11 ***\nx           -0.53615    0.06526  -8.216 2.71e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5214 on 198 degrees of freedom\nMultiple R-squared:  0.2542,    Adjusted R-squared:  0.2505 \nF-statistic:  67.5 on 1 and 198 DF,  p-value: 2.706e-14\n\n\n\ndata.frame(estimate=round(fit4b$coefficients, 2), \n           true=c(beta0, beta2))\n\n            estimate true\n(Intercept)     0.53    0\nx              -0.54    1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod2.html#homework-2",
    "href": "linmod2.html#homework-2",
    "title": "6  Linear models (part 2)",
    "section": "6.3 Homework 2",
    "text": "6.3 Homework 2\nWolff and Gorb (2013) studied the different frictional coefficients on the different legs of a spider.\nPlease, read the article, with particular focus on the analysis described in its Figure 4.\nBriefly, the goal is to compare the pulling and pushing motions of different leg pairs.\nLoad the data available in this csv file.\nPerform the following analyses:\n\nVisual inspection of the data: use a boxplot or similar plot to compare the distribution of the different forces (pull and push) and legs.\nSpecify a linear model with friction as response and type as covariate. What is the effect of the force type on friction?\nSpecify a linear model with friction as response and type and leg as covariates. What is the effect of the force type on friction? Did it change compared to the previous model? What is the effect of leg?\nSpecify a contrast to compare leg pairs L3 and L2. Is the difference in friction significant?\nSpecify a linear model that includes, in addition to the two covariates, their interaction. Is the interaction significant? How do you interpret the coefficients?\nSpecify a contrast to compare the push and pull force of leg L2. Is the difference significant?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod2.html#further-reading",
    "href": "linmod2.html#further-reading",
    "title": "6  Linear models (part 2)",
    "section": "6.4 Further reading",
    "text": "6.4 Further reading\n\nR. A. Irizarry and M. I. Love. Data Analysis for the Life Sciences",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod2.html#references",
    "href": "linmod2.html#references",
    "title": "6  Linear models (part 2)",
    "section": "References",
    "text": "References\n\n\nWolff, Jonas O, and Stanislav N Gorb. 2013. “Radial Arrangement of Janus-Like Setae Permits Friction Control in Spiders.” Scientific Reports 3 (1): 1101.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "linmod2.html#footnotes",
    "href": "linmod2.html#footnotes",
    "title": "6  Linear models (part 2)",
    "section": "",
    "text": "This lab was inspired by Regression Models for Data Science in R↩︎\nThis homework is from Data Analysis for the Life Sciences. Please try to solve this yourself before looking at the solution there.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear models (part 2)</span>"
    ]
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "7  Experimental design",
    "section": "",
    "text": "7.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experimental design</span>"
    ]
  },
  {
    "objectID": "design.html#lab",
    "href": "design.html#lab",
    "title": "7  Experimental design",
    "section": "7.2 Lab",
    "text": "7.2 Lab",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experimental design</span>"
    ]
  },
  {
    "objectID": "design.html#homework",
    "href": "design.html#homework",
    "title": "7  Experimental design",
    "section": "7.3 Homework",
    "text": "7.3 Homework",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experimental design</span>"
    ]
  },
  {
    "objectID": "design.html#further-reading",
    "href": "design.html#further-reading",
    "title": "7  Experimental design",
    "section": "7.4 Further reading",
    "text": "7.4 Further reading\n\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 13.\nNature Methods Points of Significance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experimental design</span>"
    ]
  },
  {
    "objectID": "design.html#references",
    "href": "design.html#references",
    "title": "7  Experimental design",
    "section": "References",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Experimental design</span>"
    ]
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "8  Generalized Linear Models",
    "section": "",
    "text": "8.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#lab-1",
    "href": "glm.html#lab-1",
    "title": "8  Generalized Linear Models",
    "section": "8.2 Lab 1",
    "text": "8.2 Lab 1\nIn the United States, there have been significant and historical declines in cigarette smoking. In the 1970s, 75% of high school seniors were smoking, that number is below 10% now. This progress is largely due to the tobacco control movement and their focus on initiatives like ending advertising to children (like Joe Camel), passing indoor smoking laws, health communication, etc.\nAccording to a recent report, overall tobacco/nicotine use increased in youths (middle school and high school students) in the United States in 2017 and 2018, despite previous years of declining use.\nThis major increase is attributed to an increase in the use of electronic cigarette (e-cigarette) products.\nThe main questions are:\n\nHow has tobacco and e-cigarette/vaping use by American youths changed since 2015?\nHow does e-cigarette use compare between males and females?\nWhat vaping brands and flavors appear to be used the most frequently?\nIs there a relationship between e-cigarette/vaping use and other tobacco use?\n\n\n8.2.1 Guided solution\n\nRead-in the data: because we focused on data wrangling on a previous lab, I suggest that you start from the already cleaned-up version of the data, that you can find here.\n\nNote that you have to use the function load().\nThe “codebook” with the explanation of the variables can be found here\nIn addition to the variables in the cookbook, some other variables have been define that sum all the e-cigarette / non-e-cigarette products.\n\nCreate plots to visualize the data and answer graphically to the questions above.\nConsider only data from 2015 and fit a model to compare current use of e-cigarettes between males and females.\n\nCompute “by hand” the Odds Ratio (OR) of the use of e-cigarettes between males and females. Who is most likely to smoke e-cigarettes in 2015? By how much?\nFit a logistic regression model. Is the difference significance? How do you interpret \\(\\beta_1\\)? What is its relationship with the OR calculated above? Is it the same?\n\nGo back to the full data, and fit a new logistic model that includes, in addition to Sex, year, and “non_ecig_ever”. If appropriate, consider including interactions. What do you conclude?\n\nBonusL\n\nUse the glmnet package to fit a lasso regression that has the current use of e-cigarettes as response and any appropriate variable in the dataset as covariates.\n\nHint: use the cv.glmnet function to perform the cross-validation and the coef(fit.cv, s = \"lambda.1se\") to obtain the coefficient estimates.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#further-reading",
    "href": "glm.html#further-reading",
    "title": "8  Generalized Linear Models",
    "section": "8.3 Further reading",
    "text": "8.3 Further reading\n\nS. Holmes and W. Huber. Modern Statistics for Modern Biology. Chapter 8.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "glm.html#footnotes",
    "href": "glm.html#footnotes",
    "title": "8  Generalized Linear Models",
    "section": "",
    "text": "This homework is one of the Open Case Studies. Please try to solve this yourself before looking at the solution there.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "hdr.html",
    "href": "hdr.html",
    "title": "9  High-dimensional regression",
    "section": "",
    "text": "9.1 Lecture Slides\nView slides in full screen",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>High-dimensional regression</span>"
    ]
  },
  {
    "objectID": "hdr.html#lab",
    "href": "hdr.html#lab",
    "title": "9  High-dimensional regression",
    "section": "9.2 Lab",
    "text": "9.2 Lab",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>High-dimensional regression</span>"
    ]
  },
  {
    "objectID": "hdr.html#homework",
    "href": "hdr.html#homework",
    "title": "9  High-dimensional regression",
    "section": "9.3 Homework",
    "text": "9.3 Homework",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>High-dimensional regression</span>"
    ]
  },
  {
    "objectID": "hdr.html#further-reading",
    "href": "hdr.html#further-reading",
    "title": "9  High-dimensional regression",
    "section": "9.4 Further reading",
    "text": "9.4 Further reading\n\nJames, Witten, Hastie, Tibshirani. An introduction to statistical learning. Chapter 6\nEfron, Hastie. Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Chapter 16",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>High-dimensional regression</span>"
    ]
  }
]