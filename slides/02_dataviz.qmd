---
title: "Data visualization"
subtitle: "Advanced Statistics and Data Analysis"
author: "Davide Risso"
format: 
    revealjs:
        theme: default
        
html-math-method:
    method: mathjax
    url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

# Introduction

## What is statistics

::: {.callout-important appearance="minimal"}
Statistics is the *art of making numerical conjectures* about puzzling questions.

Freedman et al., 1978.
:::

. . .

Statistics is the art and science of designing studies and analyzing the data that those studies produce.

. . .

Its ultimate goal is translating data into knowledge and understanding of the world around us.

. . .

In short, statistics is the art and science of learning from data.

## The role of statistics

-   **Design**: planning how to obtain data to answer the questions of interest.
-   **Description**: summarizing the data that are obtained.
-   **Inference**: making decisions and predictions based on the data.

## Sample and population

The **population** is the set of subjects in which we are interested.

. . .

The **sample** is the subset of the population for whom we have (or plan to have) data.

. . .

::: callout-note
## Examples:

-   Consumi Delle Famiglie (ISTAT). Sample: 28.000 families in all cities; population: all Italian families.
-   IPSOS exit polls. Sample: 1000 electors, population: all Italian relevant voters.
-   A census is a complete enumeration.
:::

## Descriptive and Inferential Statistics

**Descriptive statistics** refers to methods for summarizing the data. The summaries usually consist of graphs and numbers such as averages and percentages.

. . .

**Inferential statistics** refers to methods for making predictions or decisions about a population, based on data obtained from a sample of that population.

. . .

::: callout-note
## Example: confidence intervals

Of 834 people in a survey, 54.0$\%$ say they favor handgun control. Using statistical methods, we infer that there is 95$\%$ confidence that the true proportion is between 50.6$\%$ and 57.4$\%$.
:::

# Graphical and numerical summaries

## Data

The *data* consist of one or more *variables* measured/recorded on *observational units* in a *population* (cf. census) or *sample* of the population of interest.

The term *variable* refers to characteristics that differ among observational units.

```{r}
library(gapminder)
head(gapminder)
```

## Variables

We usually distinguish the following two main types of variables.

-   Quantitative / Numerical.
-   Qualitative / Categorical.

## Quantitative Variables

Quantitative variables can be

-   *Continuous* (cf. real numbers): any value corresponding to the points in an interval. E.g. blood pressure, height.
-   *Discrete* (cf. integers): countable number of values. E.g. number of events per year.

## Qualitative variables

Qualitative variables can be

-   *Nominal*: names or labels, no natural order. E.g. Sex, ethnicity, eye color
-   *Ordinal*: ordered categories, no natural numerical scale. E.g. Muscle tone, tumor grade.

## Tidy data

The concept of tidy data was introduced by Hadley Wickham in his seminal paper (Wickham, 2014).

. . .

There are three fundamental rules which make a dataset tidy:

1.  Each variable must have its own column.
2.  Each observation must have its own row.
3.  Each value must have its own cell.

. . .

The advantage of tidy data is both conceptual and practical: it allows to have consistency between datasets and to use tools designed for this data structure.

## Distribution {.smaller}

The term *distribution* refers to the *frequencies* of one or more (quantitative or qualitative) *variables* recorded on units in a population or sample of interest.

. . .

A *distribution table* is a table of 

- *absolute frequencies* (i.e., counts) or 
- *relative frequencies* (i.e., percentages) 

for the values taken on by the variable(s) in the group of interest.

. . .

*Population distribution*: 

- distribution of variables for units in a population; 
- *population parameters*: functions, i.e., summaries, of the population distribution.

. . .

*Empirical/Sample distribution*: 

- distribution of variables for units in a sample; 
- *sample statistics*: functions, i.e., summaries, of the empirical distribution.

## Example: life expectancy

```{r, echo=TRUE}
gapminder
summary(gapminder$lifeExp)
```

## Numerical and graphical summaries

As illustrated by the previous example, it is often more informative to provide a *summary* of the data rather than to look at all of them.

. . .

Data summaries can be either numerical or graphical and allow us to:

-   Focus on the *main characteristics* of the data.
-   Reveal *unusual features* of the data.
-   Summarize *relationship between variables*.

## When do we need summaries?

::: incremental
-   **Exploration**: to help you better understand and discover hidden patterns in the data.
-   **Explanation**: to communicate insights to others.
:::

## Exploratory Data Analysis (EDA)

*Exploratory Data Analysis* (EDA) is a key step of any data analysis and of statistical practice in general.

. . .

Before formally modeling the data, the dataset must be examined to get a "first impression" of the data and to reveal expected and unexpected characteristics of the data.

. . .

It is also useful to examine the data for outlying observations and to check the plausibility of the assumptions of candidate statistical models.

## Communication of the results

*Explanatory plots* (and tables) are equally important. They are often done at the end of the analysis to communicate the results to others.

. . .

The main goal is to communicate your main findings:

-   focus the attention on the important part of the plot
-   make it easy to understand the information (colors, labels)

## Combining accuracy with aesthetics

A good visualization should be aesthetically pleasing.

However, the most important aspect of data visualization is **accuracy**: a graph should never be misleading and aesthetics should never come at the price of inaccuracy.

# How to visualize data

## A first example

Suppose that a bakery produces five different types of pie and that the sales of these pies in the last three months were:

| Type      | October | November | December |
|-----------|---------|----------|----------|
| Apple     | 75      | 90       | 98       |
| Cherry    | 45      | 51       | 67       |
| Blueberry | 20      | 30       | 45       |
| Cream     | 90      | 89       | 96       |
| Pumpkin   | 22      | 100      | 20       |

## A bad plot

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(ggokabeito)
library(scales)
library(ggrepel)
library(patchwork)

theme_set(theme_minimal())

df <- data.frame(October = c(75, 45, 20, 90, 22),
                 November = c(90, 51, 30, 89, 100),
                 December = c(98, 67, 45, 96, 20),
                 type = c("Apple", "Cherry", "Blueberry", "Cream", "Pumpkin"))
df_long <- pivot_longer(df, cols=1:3, names_to = "month", values_to = "sales")

par(mfrow=c(1, 3))
pie(df$October, main = "October", col=c("red", "green", "yellow", "cyan", "blue"))
pie(df$November, main = "November", col=c("red", "green", "yellow", "cyan", "blue"))
pie(df$December, main = "December", col=c("red", "green", "yellow", "cyan", "blue"))
par(mfrow=c(1, 1))
```

## A differently bad plot

```{r}
#| echo: false

ggplot(df_long, aes(x = type, y = sales, fill = type)) +
    geom_bar(stat = "identity") +
    theme_gray() +
    theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank())
```

## A better plot

```{r}
#| echo: false

ggplot(df_long, aes(x = type, y = sales, group = month, fill = month)) +
    geom_bar(stat = "identity", position = "dodge") +
    theme_minimal() +
    scale_fill_okabe_ito()
```

## A better plot

```{r}
#| echo: false

df_long$month <- factor(df_long$month, levels=c("October", "November", "December"))

df_long$type <- factor(df_long$type, levels=c("Cream", "Apple", "Cherry", "Pumpkin", "Blueberry"))

ggplot(df_long, aes(y = type, x = sales, group = rev(month), fill = month)) +
    geom_bar(stat = "identity", position = "stack") +
    theme_minimal(base_size = 20) +
    scale_fill_okabe_ito()
```

## A plot that chooses what to highlight

```{r}
#| echo: false

df_long$thanksgiving <- factor(ifelse(df_long$type=="Pumpkin", "yes", "no"), levels = c("yes", "no"))
df_long <- mutate(df_long, label = if_else(month == "December", as.character(type), NA_character_))

ggplot(df_long, aes(x = month, y = sales, group = type, color = thanksgiving)) +
    geom_line(linewidth = 2) +
    geom_text_repel(aes(label = label), nudge_x = 0.1,
                  na.rm = TRUE, size = 8) +
    theme_minimal(base_size = 20) +
    theme(legend.position = "none") +
    scale_color_okabe_ito()
```

## The Grammar of Graphics

-   **Aesthethics** (`aes()`): translates the data into the *position*, *shape*, *size*, *color*, and *type* of the elements of a plot.

. . .

-   **Geometry** (`geom_*()`): selects the type of plot that will be displayed, e.g., lines, points, bars or heatmaps.

## The Grammar of Graphics (`ggplot2`) {auto-animate="true"}

In R, the [`ggplot2`](https://ggplot2-book.org/) package allows you to translate the theory into practice.

```{r}
#| eval: false
#| echo: true

ggplot(df_long, aes(x = type, y = sales, color = month))
```

## The Grammar of Graphics (`ggplot2`) {auto-animate="true"}

In R, the [`ggplot2`](https://ggplot2-book.org/) package allows you to translate the theory into practice.

```{r}
#| echo: true
#| output-location: fragment
#| fig-align: center
#| out-width: "75%"

ggplot(df_long, aes(x = type, y = sales, color = month)) +
    geom_point()
```


# Visualizing distributions

## Visualizing distributions

Often, we want to understand what is the empirical distribution of a variable in a dataset.

. . .

Perhaps we want to understand if the Gaussian distribution is a good approximation for our data, or we want to see whether the observations that we collected are uniform across age groups, etc.

. . .

In this case, we need a way to **estimate** the variable's distribution and then visualize it.

. . .

For the univariate case, the two most popular choices are *histograms* and *kernel density estimates*.

## Histograms {.smaller}

A *histogram* is used to display numerical variables, by aggregating similar values into "bins" to represent the distribution.

. . .

This can be easily done with the following steps:

1.  Partition the real line into intervals (*bins*).
2.  Assign each observation to its bin depending on its numerical value.
3.  Possibly scale the height of the bins so that the total area of the histogram is 100%

. . .

Step 3 ensures that the area of each bin represents the percentage of observations in that block.

## Example: Life expectancy

```{r}
ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(fill = "dodgerblue")
```

## How to choose the bins

The histogram representation of the data depends on two critical choices:

-   The number of bins / the bin width
-   The location of the bin boundaries.

. . .

Note that the number of bins in the histogram controls the amount of information loss: *the larger the bin width the more information we lose*.

## Example: Life expectancy

```{r, echo=FALSE}
p1 <- ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(fill = "dodgerblue", bins = 5) +
    ggtitle("bins=5")
p2 <- ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(fill = "dodgerblue", bins = 10) +
    ggtitle("bins=10")
p3 <- ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(fill = "dodgerblue", bins = 30) +
    ggtitle("bins=30")
p4 <- ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(fill = "dodgerblue", bins = 100) +
    ggtitle("bins=100")

(p1 + p2) / (p3 + p4)
```

## Density Plots

Density plots are smoothed versions of histograms, obtained using *kernel density estimation* methods.

## Density Plots

```{r, echo=FALSE}
ggplot(gapminder, aes(x=lifeExp)) +
    geom_histogram(aes(y = after_stat(density)), fill = "dodgerblue") +
    geom_density(linewidth = 2)
```

## Density Plots

The choice of *bandwidth*, similarly to the number of bins in a histogram, determines the smoothness of the density.

There is a *bias-variance trade-off* in the choice of the bandwidth:

-   the larger the bandwidth, the smoother the density (low variance, high bias)
-   the smaller the bandwidth, the less smooth (high variance, low bias)

## Density Plots

```{r, echo=FALSE}
b <- c(10, 5, 3, 1, .5)

ggplot(gapminder, aes(x=lifeExp)) +
    geom_density(bw = b[1], linewidth = 2) +
    ggtitle("bw = 10")
```

## Density Plots

```{r, echo=FALSE}
b <- c(10, 5, 3, 1, .5)

ggplot(gapminder, aes(x=lifeExp)) +
    geom_density(bw = b[1], linewidth = 2) +
    geom_density(bw = b[2], linewidth = 2, col = "blue") +
    ggtitle("bw = 5") +
    theme(plot.title = element_text(color = "blue"))
```

## Density Plots

```{r, echo=FALSE}
b <- c(10, 5, 3, 1, .5)

ggplot(gapminder, aes(x=lifeExp)) +
    geom_density(bw = b[1], linewidth = 2) +
    geom_density(bw = b[2], linewidth = 2, col = "blue") +
    geom_density(bw = b[3], linewidth = 2, col = "purple") +
    ggtitle("bw = 3") +
    theme(plot.title = element_text(color = "purple"))
```

## Density Plots

```{r, echo=FALSE}
b <- c(10, 5, 3, 1, .5)

ggplot(gapminder, aes(x=lifeExp)) +
    geom_density(bw = b[1], linewidth = 2) +
    geom_density(bw = b[2], linewidth = 2, col = "blue") +
    geom_density(bw = b[3], linewidth = 2, col = "purple") +
    geom_density(bw = b[4], linewidth = 2, col = "red") +
    ggtitle("bw = 1") +
    theme(plot.title = element_text(color = "red"))
```

## Density Plots

```{r, echo=FALSE}
b <- c(10, 5, 3, 1, .5)

ggplot(gapminder, aes(x=lifeExp)) +
    geom_density(bw = b[1], linewidth = 2) +
    geom_density(bw = b[2], linewidth = 2, col = "blue") +
    geom_density(bw = b[3], linewidth = 2, col = "purple") +
    geom_density(bw = b[4], linewidth = 2, col = "red") +
    geom_density(bw = b[5], linewidth = 2, col = "orange") +
    ggtitle("bw = 0.5") +
    theme(plot.title = element_text(color = "orange"))
```

## Numerical Summaries of One Variable

```{r, echo=FALSE}
set.seed(1220)
x3 <- data.frame(X1 = rnorm(10000,0,1),
                 X2 = rnorm(10000,5,1),
                 X3 = rnorm(10000,0,2),
                 X4 = rchisq(10000,2)-2,
                 X5 = rt(10000, df=3))

p_five <- ggplot(x3, aes(x = X1)) +
    geom_density(linewidth = 2) +
    geom_density(aes(x = X2), linewidth = 2, col = 2) +
    geom_density(aes(x = X3), linewidth = 2, col = 3) +
    geom_density(aes(x = X4), linewidth = 2, col = 4) +
    geom_density(aes(x = X5), linewidth = 2, col = 5) +
    scale_x_continuous(limits = c(-10, 15), name = "x")
p_five
```

## Numerical Summaries of One Variable

Can we describe the differences between these five distributions using numerical summaries?

. . .

::: {.incremental}
1.  *Location*: what is the "most common" value of the distribution?
2.  *Scale*: how concentrated are the values aroung the average?
3.  *Shape*: what is the shape of the distribution?
:::

## Location: Mean and Median {.smaller}

The first numerical summary relates to the location of the distribution.

The simplest summary is the (arithmetic) *mean*.

Given a list of $n$ numbers, $\{x_1, \ldots, x_n\}$

$$
\bar{x} = \frac{\sum_{i=1}^n x_i}{n}.
$$

. . .

A *robust alternative* is the *median*, which is simply the central observation. One can simply compute the median by ordering the observations and selecting the one that falls in the middle.

The *median* is a *robust summary*, in the sense that it is less influence by extreme values.

## Example: Mean vs Median

```{r}
#| echo: true
x <- 1:10
mean(x)
median(x)

x <- c(1:9,100)
mean(x)
median(x)
```

## Example: Location of the five distributions

```{r, echo=FALSE}
#| fig.align: "center"

p_five
```

The means of the five distributions are: `r round(mean(x3[,1]), 1)`, [`r round(mean(x3[,2]), 1)`]{style="color:#DF536B"}, [`r round(mean(x3[,3]), 1)`]{style="color:#61D04F"}, [`r round(mean(x3[,4]), 1)`]{style="color:#2297E6"}, [`r round(mean(x3[,5]), 1)`]{style="color:#28E2E5"}.

## Scale: Standard Deviation and Variance {.smaller}

The *standard deviation* (SD) measures the *spread* or *scale* of a distribution.

It is defined as

$$
s_x = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n -1}}.
$$

The *variance* is the square of the standard deviation.

. . .

If the data are approximately normally distributed, roughly 68% of the observations are within one SD of the mean and 95% within two SD.

## Scale: Robust Alternatives

Similarly to the median for the mean, there are robust summaries to describe the scale of the distribution.

. . .

The *inter-quartile range* (IQR) is the difference between the upper- and lower-quartile, i.e., the observation that has $75\%$ of the data below it and the observation that has $25\%$ of the observations below it, respectively.

. . .

The *median absolute deviation* (MAD) is the median of the absolute deviations from the median:

$$
MAD_x = \text{median}( | x_i - M_x |)
$$ where $M_x$ is the median of $\{x_1, \ldots, x_n\}$.

## Quantiles

The median, upper-quartile, and lower-quartile are examples of *quantiles*.

. . .

More formally, the $f$ *quantile* or $f\times 100$th *percentile* of a distribution is the smallest number $q$ such that at least $f \times 100$% of the observations are less than or equal to $q$.

. . .

In other words, $f\times 100$% of the area under the histogram is to the left of the $f$ quantile.

::: callout-note
## Quartiles

|                        |               |
|------------------------|---------------|
| First/lower quartile   | 0.25 quantile |
| Second quartile/median | 0.50 quantile |
| Third/upper quartile   | 0.75 quantile |

:::

## The Median as a Quantile

The *median* corresponds to the special case $f=0.5$.

If the number of observations $n$ is *odd*, i.e., $n=2m+1$, there will be an observation that corresponds to the median.

If the number of observations $n$ is *even*, i.e., $n=2m$, we need to average the two central observations.

## Boxplot

The boxplot, also called *box-and-whisker plot* was first introduced by Tukey in 1977 as a graphical summary of a variable's distribution.

It is a graphical representation of the median, upper and lower quartile, and the range (possibly, with outliers).

## Boxplot

```{r}
#| fig-width: 4
#| fig-height: 5
#| fig-align: center

ggplot(gapminder, aes(y = lifeExp)) +
    geom_boxplot(fill = "dodgerblue")
```

## Boxplot

-   The bold line represents the *median*.
-   The upper and lower sides of the box represent the lower and upper quartiles, respectively.
-   The central box represents the *IQR*.
-   The whiskers represent the range of the variable, but any point more than 1.5 IQR above the upper quartile (or below the lower quartile) are plotted individually as *outliers*.
-   Comparing the distances between the quartiles and the median gives indication of the symmetry of the distribution.

## Boxplots of the five distributions

```{r, echo=FALSE}
x3_long <- pivot_longer(x3, cols=1:5, names_to = "distribution")
ggplot(x3_long, aes(y = value, x = distribution)) +
    geom_boxplot(fill = "dodgerblue")
```

## Pros and Cons of Boxplots

::: {.incremental}

- Boxplots are a great graphical tool to summarize distributions, especially when they are approximately normally distributed.

- Even in the presence of strong asymmetry, the boxplot still captures the shape of the distribution.

- One of the advantages of the boxplot is its effectiveness at identifying outlying observations.

- However, the boxplot *hides multimodality* in the data.

:::

## Pros and Cons of Boxplots

```{r, echo=FALSE}
set.seed(1221)

x <- rnorm(1000)
y <- rgamma(1000, shape = .5)
z <- c(rnorm(500), rnorm(500, mean=5))

x4 <- data.frame(normal=x, skewed=y, bimodal=z)
x4_long <- pivot_longer(x4, cols = 1:3)
x4_long$name <- factor(x4_long$name, levels = c("normal", "skewed", "bimodal"))
p0 <- ggplot(x4_long, aes(y = value, x = name)) +
    geom_boxplot(fill = "dodgerblue") +
    theme(axis.title = element_blank())

p4 <- ggplot(x4, aes(x = normal)) +
    geom_histogram(aes(y = after_stat(density)), fill="dodgerblue") +
    geom_density(linewidth = 2)

p5 <- ggplot(x4, aes(x = skewed)) +
    geom_histogram(aes(y = after_stat(density)), fill="dodgerblue") +
    geom_density(linewidth = 2)

p6 <- ggplot(x4, aes(x = bimodal)) +
    geom_histogram(aes(y = after_stat(density)), fill="dodgerblue") +
    geom_density(linewidth = 2)

p0 / (p4 + p5 + p6)
```

## Scale of the five distributions

```{r, echo=FALSE}
#| fig.align: "center"
library(moments)
p_five
```

The standard deviations of the five distributions are: `r round(sd(x3[,1]))`, [`r round(sd(x3[,2]))`]{style="color:#DF536B"}, [`r round(sd(x3[,3]))`]{style="color:#61D04F"}, [`r round(sd(x3[,4]))`]{style="color:#2297E6"}, [`r round(sd(x3[,5]))`]{style="color:#28E2E5"}.

## Shape of the five distributions

::: {.incremental}

- Even though the green, blue, and cyan distributions have the same mean and variance, they are clearly different.

- How can we capture these differences?

- There are numerical summaries called *skewness* and *kurtosis*, but the most effective way is to look at graphical summaries (boxplots and histograms).

- The skewness of the five distributions are: `r round(skewness(x3[,1]))`, [`r round(skewness(x3[,2]))`]{style="color:#DF536B"}, [`r round(skewness(x3[,3]), 1)`]{style="color:#61D04F"}, [`r round(skewness(x3[,4]))`]{style="color:#2297E6"}, [`r round(skewness(x3[,5]))`]{style="color:#28E2E5"}.

- The kurtosis of the five distributions are: `r round(kurtosis(x3[,1]))`, [`r round(kurtosis(x3[,2]))`]{style="color:#DF536B"}, [`r round(kurtosis(x3[,3]))`]{style="color:#61D04F"}, [`r round(kurtosis(x3[,4]))`]{style="color:#2297E6"}, [`r round(kurtosis(x3[,5]))`]{style="color:#28E2E5"}.

:::

## Alternatives to boxplots

Boxplots can be used to visualize multiple distributions at once.

If we want to do the same using kernel density, we can use _violin plots_ or _ridgeline plots_.

## Violin plots

```{r}
ggplot(x4_long, aes(y = value, x = name)) +
    geom_violin(fill = "dodgerblue")
```

## Ridgeline plots

```{r}
library(ggridges)
ggplot(x4_long, aes(x = value, y = name)) +
    geom_density_ridges(fill = "dodgerblue")
```

# Visualizing two variables

## Graphical Summaries of Two Variables

::: {.incremental}

- Boxplots, histograms, and density plots all summarize the univariate distribution of a single variable. But what if we want to capture the relation between variables?

- The most used graphical representation of the relation between two variables is not a summary. We can simply plot _all the observations_ for the two variables in a _scatterplot_.

- When there are too many observations, summaries based on 2D-density estimation can be used.

:::

## Scatterplots

```{r, echo=FALSE, warning=FALSE}
p1 <- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +
    geom_point() +
    scale_x_continuous(trans = "log10")
p1
```


## Considerations for scatterplots

Even for this simple element, there are important considerations to make when presenting results.

. . .

In fact, "stretching" one or the other axis may lead to emphasizing one aspect over others.

. . .

This is particularly important in the case where the measurements of the two axes are similar and should be treated as such.

. . . 

::: callout-tip
Use the `coord_equal()` function in `ggplot2` to ensure equal coordinates.
:::

## Coordinate systems and axes

```{r}
#| echo: false
(p1 + p1 + p1) / p1
```

## Data transformation

You may have noticed that I used the logarithm of GDP per capita in the previous plot.

. . .

This is because in the linear scale this variable has a skewed distribution most points are "squished" to the left, making the general trend difficult to see.

. . .

Log transformation, like many data transformations, can be useful, but sometimes can create distorsions; hence, it is always good to try different scales for your data *in the exploratory phase*.

## Example {auto-animate="true"}

```{r}
#| echo: true

ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +
    geom_point()
```

## Example {auto-animate="true"}

```{r}
#| echo: true

ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +
    geom_point() +
    scale_x_continuous(trans = "log10")
```

## Avoid overplotting

When there are too many data points, plotting every point is redundant and may cause loss of information.

In such cases, it is better to display the two-dimensional density of the points, either with contours or with hexagonal binning.

## Example {.smaller}

```{r}
library("Hiiragi2013")
data("x")
```

Let's have a look at an example dataset, which contains the expression of `r nrow(x)` genes in two embryos.

Here, we are plotting tens of thousands of points, many of which are on top of each other.

```{r}
#| fig-align: center
dfx <- as.data.frame(Biobase::exprs(x))
scp <- ggplot(dfx, aes(x = `59 E4.5 (PE)` ,
                      y = `92 E4.5 (FGF4-KO)`))
scp + geom_point() + coord_fixed()
```

## Example {.smaller}

We can estimate a 2D density and plot the density function instead of (or in addition to) the points.

```{r}
#| fig-align: center
scp + geom_density_2d(h = 0.5, bins = 60) + coord_fixed()
```

## Example {.smaller}

In this case a better alternative is to use hexagonal binning of points to represent their density.

```{r}
#| fig-align: center
scp + geom_hex(binwidth = c(0.2, 0.2)) +
    scale_fill_viridis_c() +
    coord_fixed()
```

## Numerical Summaries of Two Variables

The variance and standard deviation are summaries of the spread of the distribution of one variable.

. . .

We can compute them for multiple variables, but they will not tell
us anything about the relationship between two or more variables.

. . .

The _covariance_ can be used to assess the relation between two variables, say $x$ and $y$.

$$
Cov(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}).
$$

## Correlation Coefficient {.smaller}

The problem with the covariance is that it depends on the scale of the two variables. It is often more useful to have an absolute indication of the strength of the relation, such as an index between -1 and 1.

. . .

Such an index is the _correlation coefficient_.

It is a measure of the _linear association_ between two variables.

$$
r = Cor(x, y) = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i - \bar{x}}{s_x} \right) \left( \frac{y_i - \bar{y}}{s_y} \right)
$$

This is also called _Pearson correlation_.

. . .

_Spearman's correlation coefficient_ is a robust measure of correlation, where ranks are used in place of the values of $x$ and $y$.

## Example of Correlated and Uncorrelated Data

```{r, echo=FALSE}
suppressPackageStartupMessages(library(MASS))

set.seed(1447)
x <- mvrnorm(100,c(0,0),matrix(c(1,0,0,1),2,2,byrow=TRUE))
dfx1 <- data.frame(x)
p1 <- ggplot(dfx1, aes(X1, X2)) + geom_point()

x <- mvrnorm(100,c(0,0),matrix(c(1,-0.75,-0.75,1),2,2,byrow=TRUE))
dfx2 <- data.frame(x)
p2 <- ggplot(dfx2, aes(X1, X2)) + geom_point()

x <- mvrnorm(100,c(0,0),matrix(c(1,0.75,0.75,1),2,2,byrow=TRUE))
dfx3 <- data.frame(x)
p3 <- ggplot(dfx3, aes(X1, X2)) + geom_point()

x <- mvrnorm(100,c(0,0),matrix(c(1,0.99,0.99,1),2,2,byrow=TRUE))
dfx4 <- data.frame(x)
p4 <- ggplot(dfx4, aes(X1, X2)) + geom_point()

(p1 + p2) / (p3 + p4)
```

## These patterns are not captured by univariate techniques

```{r, echo=FALSE}
dfl1 <- pivot_longer(dfx1, cols = 1:2, names_to = "variable")
p1 <- ggplot(dfl1, aes(variable, value)) + geom_boxplot()

dfl2 <- pivot_longer(dfx2, cols = 1:2, names_to = "variable")
p2 <- ggplot(dfl2, aes(variable, value)) + geom_boxplot()

dfl3 <- pivot_longer(dfx3, cols = 1:2, names_to = "variable")
p3 <- ggplot(dfl3, aes(variable, value)) + geom_boxplot()

dfl4 <- pivot_longer(dfx4, cols = 1:2, names_to = "variable")
p4 <- ggplot(dfl4, aes(variable, value)) + geom_boxplot()

(p1 + p2) / (p3 + p4)
```

# Visualizing more than two variables

## Faceting

The embryo gene expression example dataset is much more complex than what we have been looked at earlier.

```{r}
library("Hiiragi2013")
data("x")
```

It contains  `r nrow(x)` genes measured in `r ncol(x)` samples.

. . .

In some cases, it may be useful to use _faceting_ to represent a third dimension, e.g., this plot shows the relation between two genes across samples, stratified by genotype.

## Faceting

```{r}
library(magrittr)
dftx <- data.frame(t(Biobase::exprs(x)), pData(x))

dftx$lineage %<>% sub("^$", "no", .)
dftx$lineage %<>% factor(levels = c("no", "EPI", "PE", "FGF4-KO"))

ggplot(dftx, aes(x = X1426642_at, y = X1418765_at)) +
  geom_point() + facet_grid( . ~ lineage ) +
    theme_bw()
```

## Heatmaps {.smaller}

Heatmaps can be used to visualize high-dimensional data.

```{r}
#| fig-align: center
#| fig-width: 6
#| fig-height: 5
library("pheatmap")
topGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]
rowCenter = function(x) { x - rowMeans(x) }
dfx <- as.data.frame(Biobase::exprs(x))

groups <- group_by(pData(x), sampleGroup) |>
  summarise(n = n(), color = unique(sampleColour))
groupColor <- setNames(groups$color, groups$sampleGroup)


pheatmap(rowCenter(dfx[topGenes, ]), 
  show_rownames = FALSE, 
  show_colnames = FALSE, 
  breaks = seq(-5, +5, length = 101),
  annotation_col = pData(x)[, c("sampleGroup", "Embryonic.day", "ScanDate", "genotype") ],
  annotation_colors = list(
    sampleGroup = groupColor,
    genotype = c(`FGF4-KO` = "chocolate1", `WT` = "azure2"),
    Embryonic.day = setNames(brewer.pal(9, "Blues")[c(3, 6, 9)], c("E3.25", "E3.5", "E4.5")),
    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), "YlGn"), levels(x$ScanDate))
  )
)
```

## Dimensionality reduction {.smaller}

A smart alternative for high-dimensional data is to project the data points onto a lower-dimensional embedding, such as the space of the first principal components.

```{r}
pca <- prcomp(t(dfx))
pcadf <- cbind(pca$x[,1:10], pData(x))

ggplot(pcadf, aes(PC1, PC2, color = genotype, shape = Embryonic.day)) +
    geom_point(size=2) +
    coord_fixed()
```
