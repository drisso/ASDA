---
title: "Experimental design"
subtitle: "Advanced Statistics and Data Analysis"
author: "Davide Risso"
format: 
    revealjs:
        theme: default
        incremental: true
html-math-method:
    method: mathjax
    url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
knitr:
    opts_chunk:
        out-width: 75%
        fig-align: center
---


# Introduction

## Experimental design

:::{.callout-tip}

# Quote

"To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of."

R.A. Fisher (1938)
:::

## What is experimental design?

Experimental design is the field of statistics dealing with how to collect data so that the results will best be able to answer the question of interest.

. . .

Generally, we want to compare/manipulate: e.g. try two different scenarios and see the effect on a certain response variable.

. . .

The term "Experiment" implies labs, test tubes, etc. but it is much broader. We refer to "experiments" whenever there is a choice by the investigator to assign one of multiple treatments to each unit in the study.

## What is experimental design?

[Holmes and Huber](https://www.huber.embl.de/msmb/13-chap.html#sec-design-typesexps) call experimental design _the art of good enough_.

- Experimental design rationalizes the tradeoffs imposed by having finite resources.
- Sample sizes are limited for practical, economic, and sometimes ethical reasons.
- Our measurements may be overlaid with nuisance factors over which we have limited control.
- There is little point in prescribing unrealistic ideals: we need to make pragmatic choices that are feasible.

## Types of experiments {.smaller}

In order on how much we can control.

- **Experiment** (proper)
    - Everyting is controlled

- **Prospective, controlled studies**
    - E.g., clinical trials
    - randomization, blinding
    - ethical constraints

- **Retrospective observational studies**: we lack control on 
    - study participants' behavior
    - assignment of factors
    - confounding

    
- **Meta analysis**
    - Retrospective analysis of data that has already been collected
    
## Is this statistics?

- In order to design a sensible experiment, one must know how they will analyze the data.

- The design of an experiment is based on statistical properties of the analysis (and data).

- Understanding the analysis will lead to understanding how the data should best be collected.

- **In order to correctly analyze, we must understand the design**.

## Is this statistics?

- Much of applied statistics involves more than just analysis.

- Even if you are not involved in the design (or there is no design at all) experimental design can help analyze the data.

- What would a well-designed experiment look like? 

- What can/cannot answer with this design? (Maybe nothing at all!)

# Confounding

## A toy example

- Assume that we want to study whether a certain _treatment_ affect a certain protein expression.

- We may design an experiment in which we give the treatment to a certain number of mice, say six, and measure the protein expression.

- To minimize variability, all mice come from the same litter.

- What can we conclude from the results in the next slide?

## A toy example

```{r}
#| fig-width: 4
#| fig-height: 5
#| fig-align: center

library(tidyverse)
theme_set(theme_minimal(base_size = 20))

set.seed(1520)

expression <- c(rnorm(6, mean=1.5), rnorm(6, mean=4))
treatment <- factor(rep(c("treated", "control"), each=6),
                    levels = c("treated", "control")) 
litter <- factor(rep(c("litterA", "litterB"), each=6),
                    levels = c("litterA", "litterB")) 

df <- data.frame(expression, treatment, litter)

df |>
    filter(treatment == "treated") |>
    ggplot(aes(y = expression, x = 1)) +
    geom_boxplot() +
    geom_point(aes(color = treatment), size = 3)
```

## A toy example (with controls)

- We need a reference point to understand if the expression is high or low.

- We can measure the protein expression on another six mice (from another litter), to which we did not give the treatment.

- What can we conclude from the results in the next slide?

## A toy example (with controls)

```{r}
df |>
    ggplot(aes(y = expression, x = treatment)) +
    geom_boxplot() +
    geom_point(aes(color = treatment), size = 3)
```

## A toy example (with controls)

```{r}
summary(lm(expression ~ treatment))
```

## Are you sure?

- We need a reference point to understand if the expression is high or low.

- We can measure the protein expression on another six mice (**from another litter**), to which we did not give the treatment.

- What can we **really** conclude from the results in the next slide?


## A toy example (with confounding)

```{r}
df |>
    ggplot(aes(y = expression, x = treatment)) +
    geom_boxplot() +
    geom_point(aes(color = litter), size = 3) +
    scale_color_brewer(type = "qual")
```

## A toy example (with confounding)

```{r}
summary(lm(expression ~ treatment + litter, data=df))
```

## A more realistic example

- [Lin et al. (2014)](https://www.pnas.org/doi/10.1073/pnas.1413624111) studied the gene expression of several tissues across mice and humans

- The question they asked was whether the transcriptome is more similar for a tissue across species or for a species across tissues.

- Perhaps surprisingly, the study concluded that "differences dominate similarities between the two species [...] likely reflecting the fundamental physiological differences between these two organisms".

## A more realistic example

![](img/lin_pnas1.png){fig-align="center"}

## A more realistic example

[Gilad & Mizrahi-Man](https://f1000research.com/articles/4-121) reanalyzed the data and found that _species is confounded with sequencing batch_.

![](img/gilad1.gif){fig-align="center"}

## A more realistic example

If we adjust for the "batch effect", we remove the species difference, but is this real?

![](img/gilad2.png){fig-align="center"}

## What can we conclude?

![](https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExOGM2Y3VwcXFiemVwZTE0Mmhybzl6MGR1d2xmZmZqZHpsbWJqbzdjaiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/K6VhXtbgCXqQU/giphy.gif)

# Principles of experimental design

## The steps of experimental design

1. **Design** -- choices you make _before_ collecting the data
    - What measurement to make (the response)?
    - What conditions to compare (the treatment)?
    - What is a unit that will get a treatment (individual person? groups of people, like a hospital?)
    - How many samples do I need?
    - Which units get which (combination) of treatments?

2. **Running the experiment**

3. **Analysis** -- how you analyze the data created from a particular design to answer the scientific question.

## Back to the toy example

- What measurement to make (the response)?
- What conditions to compare (the treatment)?
- What is a unit that will get a treatment?
- How many samples do I need?
- Which units get which (combination) of treatments?

## Better ways to design the experiment

Now that we know that litter may cause confounding, we have three options.

1. _Reduce technical variance_: we use for our experiments only mice from the same litter.
2. _Randomization_: instead of assigning all mice from one litter to each treatment, we randomize the order.
3. _Blocking_: we take pairs of mice from several litters and give the treatment to one of the two, randomly.

. . .

**What are the pros and cons of each option?**

## A better experiment: randomization

```{r}
set.seed(1632)

df <- mutate(df, litter = rep(paste0("litter", LETTERS[1:2]), 6))
df <- mutate(df, expression = c(rnorm(6, mean=1.5, sd=2), rnorm(6, mean=4, sd=2)))
df |>
    ggplot(aes(y = expression, x = treatment)) +
    geom_boxplot() +
    geom_point(aes(color = litter), size = 3) +
    scale_color_brewer(type = "qual")
```

## A better experiment: randomization

```{r}
summary(lm(expression ~ treatment + litter, data=df))
```

## A better experiment: blocking

```{r}
set.seed(1655)

x1 <- rnorm(6, mean=2)
x2 <- x1 + rnorm(6, mean=.7, sd=0.1)
df <- mutate(df, expression = c(x1, x2))
df <- mutate(df, litter = rep(paste0("litter", LETTERS[1:6]), 2))

df |>
    ggplot(aes(y = expression, x = treatment, color = litter)) +
    geom_point(size = 3) +
    geom_line(aes(group = litter)) +
    scale_color_brewer(type = "qual")
```


## A better experiment: blocking

```{r}
summary(lm(expression ~ treatment + litter, data=df))
```

## A better experiment: blocking

If we don't include litter, we lose power, but the treatment effect is correctly estimated.

```{r}
summary(lm(expression ~ treatment, data=df))
```

## Side note {.smaller}

**This is exactly the difference between a two-sample and a paired t-test!**

```{r}
df_wide <- pivot_wider(df, names_from = "treatment", values_from = "expression")
t.test(df_wide$treated, df_wide$control)
t.test(df_wide$treated, df_wide$control, paired=TRUE)
```

## What have we done?

An experiment like this is called **randomized block design**.

. . .

The variable _litter_ is the **blocking variable**.

. . .

Each treatment is applied within each block.

. . .

_Randomization_ and _blocking_ are the two main devices of experimental design.

# Randomization vs Blocking

## Randomization vs Blocking

:::{.callout-tip}

# Quote

"Block what you can, randomize what you cannot."

George Box (1978)
:::

In a **Randomized Complete Block Design** the _experimental units_ are blocks sampled at random from the population.

Treatments are then assigned at random to the _observational units_ within each block.

## Randomized Complete Block Designs

**Each treatment is applied to each block exactly once**.

![](img/CompleteRB3.png){fig-algin="center"}

Paired designs are a special case with two treatments.

## Completely Randomized Design

- Sometimes it is not possible or practicle to block, e.g., \
    - we do not know the nuisance factor in advance
    - we cannot assign a treatment to a block more than once
    
- In such cases, we can use _randomization_, i.e., assign the treatment randomly to each experimental unit.

- Randomization decreases (unconscious) bias and helps with _unknown_ nuisance factors.

## Blocking leads to more power

- We have seen in the mouse toy example that blocking for the litter effect led to a smaller p-value.

- In general, blocking leads to **more power** than randomization.

- We can simulate some data to empirically show this effect.

## Simulation study

```{r, echo=TRUE}
set.seed(1547)
B <- 1000
alpha <- 0.05
n <- 15
effect <- 0.2
pvals <- replicate(B, {
    litters <- rnorm(n, 0, 1)
    noiset <- rnorm(n, 0, 0.25)
    noisec <- rnorm(n, 0, 0.25)
    treated <- litters + effect + noiset
    controls  <- litters + noisec
    
    c(t.test(treated, controls, paired=FALSE)$p.value,
      t.test(treated, controls, paired=TRUE)$p.value)
})
rowMeans(pvals <= alpha)
```

## Simulation study

```{r}
pvals <- as.data.frame(t(pvals))
colnames(pvals) <- c("False", "True")
tidyr::pivot_longer(pvals, cols = everything(), names_to = "paired") |>
  ggplot(aes(x = value, fill = paired)) +
  geom_histogram(binwidth = 0.01, boundary = 0, alpha = 1/3)
```

# Replication

## How many replicates do I need?

{{< video https://www.youtube.com/watch?v=Hz1fyhVOjr4 width="100%" height="85%" >}}

## The right question to ask

- How many replicates do I need to detect _this_ effect given that the data exhibit _this_ variability?

- In other words, the power to detect a difference depends on the effect size and the variance, in addition to sample size.

- Intuitively, we need few replicates when comparing well-controlled cell lines, many more when performing an observational studies in a heterogeneous human population.

## Power calculations

```{r, echo=TRUE}
power.t.test(n = 3, delta = 1, sd = 0.5)
```

## Sample size calculations

```{r, echo=TRUE}
power.t.test(delta = 1, sd = 0.5, power = 0.9)
```
## Sample size calculation

It may not be obvious what are the expected effect size and variability.

In such cases, **pilot experiments** may be useful.

## Technical vs. biological replicates

Not all replicates are equal!

![](img/tech_bio.png){fig-align="center"}

## Experimental vs. observational units

- **Experimental units** are the smallest entities that can be _independently_ assigned to a treatment (e.g., animal, litter, ...)

- **Observational units** are the units at which measurements are made.

. . .

Think of single-cell molecular assays: we measure e.g. gene expression at the individual cell level, but we are typically interested in treatments at the animal or tissue level.

## Technical vs. biological replicates

- Biological replication is required to make a general statement of the effect of a treatment 

- You can't draw a conclusion about the effect of a compound on people in general by observing the effect in one person.

- **Only replication of experimental units is true replication (independent error terms).**

## Pseudo replication

- Only experimental units are independent.
    - E.g., cells from the same mouse are more correlated to each other than to cells from a different mouse.
    
- A sample of _independent_ observations is more informative than one of _dependent_ ones.

- Example: ask the political opinion of $n$ people randomly picked from $n$ independent locations vs. $3$ randomly picked locations and $n/3$ people randomly picked at each location.

## Nested designs

- Although less ideal than Randomized Complete Block Designs, _nested_ (or _split-plot_) designs are common and describe a situation similar to what we just described.

- Nested designs may be more convenient, for instance in education we are interested in measuring effects on children, but children are taught in classrooms, hence we often cannot apply treatments at the child level, but at the school or teacher level.


## Nested design: an example

An experiment compares the ability of two technologies to detect diabetes in dogs.
Ten dogs are randomly divided into two groups: in one group they induce diabetes.

**What kind of design can we use?**


## Nested design: an example

1. **Completely Randomized**: 2 technologies $\times$ diabetic or not, hence 4 treatment combinations: randomly assign a treatment combination to a dog. Don't we want to block for the dog effect (individual effects are important!)?

2. **Complete Block**: for each dog, randomly assign each treatment combinations. Ideal but not possible, a dog is either diabetic or not.

3. **Nested Design**: each dog gets randomly assign diabetes or not, but then measure each dog with both technology.

## Designs are about constraints

- We have a nested design because we cannot perform a complete block design.

- The consequence is that we now have pseudo-replication: we measure each dog twice, but the treatment is at the dog level.

- We will see that _random effect_ and _mixed effect_ models are one way to deal with pseudo-replication.

- One particular case of pseudo-replication is _longitudinal data_, in which each experimental unit is measured across time.




